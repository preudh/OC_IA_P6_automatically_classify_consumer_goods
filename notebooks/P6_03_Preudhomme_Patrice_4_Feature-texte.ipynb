{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 1 : Importation des librairies essentielles python",
   "id": "837f7c652c46b58d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-14T07:36:56.174382Z",
     "start_time": "2024-09-14T07:36:56.168191Z"
    }
   },
   "source": [
    "# Import des librairies essentielles\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ],
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 2 : Importation des données",
   "id": "e3d462b424487c9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:36:56.294678Z",
     "start_time": "2024-09-14T07:36:56.253393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Définir le chemin relatif\n",
    "file_path = os.path.join(\"..\", \"data\", \"flipkart_com-ecommerce_sample_1050.csv\")\n",
    "\n",
    "# Chargement du dataset des produits\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Dimensions du dataset :\", data.shape)\n"
   ],
   "id": "25500449b6b27523",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du dataset : (1050, 15)\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 3 : Affichage d'un aperçu des données",
   "id": "f8f1d994554b404a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:36:56.378693Z",
     "start_time": "2024-09-14T07:36:56.358490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Afficher les premières lignes du dataset\n",
    "data.head()"
   ],
   "id": "afad58cc0e5c8f5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                            uniq_id            crawl_timestamp  \\\n",
       "0  55b85ea15a1536d46b7190ad6fff8ce7  2016-04-30 03:22:56 +0000   \n",
       "1  7b72c92c2f6c40268628ec5f14c6d590  2016-04-30 03:22:56 +0000   \n",
       "2  64d5d4a258243731dc7bbb1eef49ad74  2016-04-30 03:22:56 +0000   \n",
       "3  d4684dcdc759dd9cdf41504698d737d8  2016-06-20 08:49:52 +0000   \n",
       "4  6325b6870c54cd47be6ebfbffa620ec7  2016-06-20 08:49:52 +0000   \n",
       "\n",
       "                                         product_url  \\\n",
       "0  http://www.flipkart.com/elegance-polyester-mul...   \n",
       "1  http://www.flipkart.com/sathiyas-cotton-bath-t...   \n",
       "2  http://www.flipkart.com/eurospa-cotton-terry-f...   \n",
       "3  http://www.flipkart.com/santosh-royal-fashion-...   \n",
       "4  http://www.flipkart.com/jaipur-print-cotton-fl...   \n",
       "\n",
       "                                        product_name  \\\n",
       "0  Elegance Polyester Multicolor Abstract Eyelet ...   \n",
       "1                         Sathiyas Cotton Bath Towel   \n",
       "2                Eurospa Cotton Terry Face Towel Set   \n",
       "3  SANTOSH ROYAL FASHION Cotton Printed King size...   \n",
       "4  Jaipur Print Cotton Floral King sized Double B...   \n",
       "\n",
       "                               product_category_tree               pid  \\\n",
       "0  [\"Home Furnishing >> Curtains & Accessories >>...  CRNEG7BKMFFYHQ8Z   \n",
       "1  [\"Baby Care >> Baby Bath & Skin >> Baby Bath T...  BTWEGFZHGBXPHZUH   \n",
       "2  [\"Baby Care >> Baby Bath & Skin >> Baby Bath T...  BTWEG6SHXTDB2A2Y   \n",
       "3  [\"Home Furnishing >> Bed Linen >> Bedsheets >>...  BDSEJT9UQWHDUBH4   \n",
       "4  [\"Home Furnishing >> Bed Linen >> Bedsheets >>...  BDSEJTHNGWVGWWQU   \n",
       "\n",
       "   retail_price  discounted_price                                 image  \\\n",
       "0        1899.0             899.0  55b85ea15a1536d46b7190ad6fff8ce7.jpg   \n",
       "1         600.0             449.0  7b72c92c2f6c40268628ec5f14c6d590.jpg   \n",
       "2           NaN               NaN  64d5d4a258243731dc7bbb1eef49ad74.jpg   \n",
       "3        2699.0            1299.0  d4684dcdc759dd9cdf41504698d737d8.jpg   \n",
       "4        2599.0             698.0  6325b6870c54cd47be6ebfbffa620ec7.jpg   \n",
       "\n",
       "   is_FK_Advantage_product                                        description  \\\n",
       "0                    False  Key Features of Elegance Polyester Multicolor ...   \n",
       "1                    False  Specifications of Sathiyas Cotton Bath Towel (...   \n",
       "2                    False  Key Features of Eurospa Cotton Terry Face Towe...   \n",
       "3                    False  Key Features of SANTOSH ROYAL FASHION Cotton P...   \n",
       "4                    False  Key Features of Jaipur Print Cotton Floral Kin...   \n",
       "\n",
       "        product_rating       overall_rating                  brand  \\\n",
       "0  No rating available  No rating available               Elegance   \n",
       "1  No rating available  No rating available               Sathiyas   \n",
       "2  No rating available  No rating available                Eurospa   \n",
       "3  No rating available  No rating available  SANTOSH ROYAL FASHION   \n",
       "4  No rating available  No rating available           Jaipur Print   \n",
       "\n",
       "                              product_specifications  \n",
       "0  {\"product_specification\"=>[{\"key\"=>\"Brand\", \"v...  \n",
       "1  {\"product_specification\"=>[{\"key\"=>\"Machine Wa...  \n",
       "2  {\"product_specification\"=>[{\"key\"=>\"Material\",...  \n",
       "3  {\"product_specification\"=>[{\"key\"=>\"Brand\", \"v...  \n",
       "4  {\"product_specification\"=>[{\"key\"=>\"Machine Wa...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>crawl_timestamp</th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_category_tree</th>\n",
       "      <th>pid</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>discounted_price</th>\n",
       "      <th>image</th>\n",
       "      <th>is_FK_Advantage_product</th>\n",
       "      <th>description</th>\n",
       "      <th>product_rating</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_specifications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7</td>\n",
       "      <td>2016-04-30 03:22:56 +0000</td>\n",
       "      <td>http://www.flipkart.com/elegance-polyester-mul...</td>\n",
       "      <td>Elegance Polyester Multicolor Abstract Eyelet ...</td>\n",
       "      <td>[\"Home Furnishing &gt;&gt; Curtains &amp; Accessories &gt;&gt;...</td>\n",
       "      <td>CRNEG7BKMFFYHQ8Z</td>\n",
       "      <td>1899.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Key Features of Elegance Polyester Multicolor ...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Elegance</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Brand\", \"v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590</td>\n",
       "      <td>2016-04-30 03:22:56 +0000</td>\n",
       "      <td>http://www.flipkart.com/sathiyas-cotton-bath-t...</td>\n",
       "      <td>Sathiyas Cotton Bath Towel</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby Bath &amp; Skin &gt;&gt; Baby Bath T...</td>\n",
       "      <td>BTWEGFZHGBXPHZUH</td>\n",
       "      <td>600.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Specifications of Sathiyas Cotton Bath Towel (...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Sathiyas</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Machine Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74</td>\n",
       "      <td>2016-04-30 03:22:56 +0000</td>\n",
       "      <td>http://www.flipkart.com/eurospa-cotton-terry-f...</td>\n",
       "      <td>Eurospa Cotton Terry Face Towel Set</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby Bath &amp; Skin &gt;&gt; Baby Bath T...</td>\n",
       "      <td>BTWEG6SHXTDB2A2Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Key Features of Eurospa Cotton Terry Face Towe...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Eurospa</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Material\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d4684dcdc759dd9cdf41504698d737d8</td>\n",
       "      <td>2016-06-20 08:49:52 +0000</td>\n",
       "      <td>http://www.flipkart.com/santosh-royal-fashion-...</td>\n",
       "      <td>SANTOSH ROYAL FASHION Cotton Printed King size...</td>\n",
       "      <td>[\"Home Furnishing &gt;&gt; Bed Linen &gt;&gt; Bedsheets &gt;&gt;...</td>\n",
       "      <td>BDSEJT9UQWHDUBH4</td>\n",
       "      <td>2699.0</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>d4684dcdc759dd9cdf41504698d737d8.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Key Features of SANTOSH ROYAL FASHION Cotton P...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>SANTOSH ROYAL FASHION</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Brand\", \"v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6325b6870c54cd47be6ebfbffa620ec7</td>\n",
       "      <td>2016-06-20 08:49:52 +0000</td>\n",
       "      <td>http://www.flipkart.com/jaipur-print-cotton-fl...</td>\n",
       "      <td>Jaipur Print Cotton Floral King sized Double B...</td>\n",
       "      <td>[\"Home Furnishing &gt;&gt; Bed Linen &gt;&gt; Bedsheets &gt;&gt;...</td>\n",
       "      <td>BDSEJTHNGWVGWWQU</td>\n",
       "      <td>2599.0</td>\n",
       "      <td>698.0</td>\n",
       "      <td>6325b6870c54cd47be6ebfbffa620ec7.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Key Features of Jaipur Print Cotton Floral Kin...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Jaipur Print</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Machine Wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 4 : Vérification des colonnes disponibles",
   "id": "a2d37e68f96172f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:36:56.520147Z",
     "start_time": "2024-09-14T07:36:56.511702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Afficher les noms des colonnes\n",
    "print(\"Colonnes du dataset :\", data.columns.tolist())\n"
   ],
   "id": "39474df71d1701f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes du dataset : ['uniq_id', 'crawl_timestamp', 'product_url', 'product_name', 'product_category_tree', 'pid', 'retail_price', 'discounted_price', 'image', 'is_FK_Advantage_product', 'description', 'product_rating', 'overall_rating', 'brand', 'product_specifications']\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 9 : Préparation des catégories",
   "id": "cd5c3e89a4afb38d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:36:56.666330Z",
     "start_time": "2024-09-14T07:36:56.649506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Extraire la première catégorie de la colonne 'product_category_tree'\n",
    "data['category'] = data['product_category_tree'].apply(lambda x: x.split('>>')[0].strip())\n",
    "\n",
    "# Mettre les catégories en minuscules pour uniformiser\n",
    "data['category'] = data['category'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remplacer les valeurs manquantes dans 'category' par 'unknown'\n",
    "data['category'] = data['category'].fillna('unknown')\n",
    "\n",
    "# Liste des catégories uniques\n",
    "categories = data['category'].unique().tolist()\n",
    "print(\"Catégories :\", categories)\n",
    "\n",
    "# Mapping des catégories vers des numéros\n",
    "category_to_num = {category: idx for idx, category in enumerate(categories)}\n",
    "data['category_num'] = data['category'].map(category_to_num)\n",
    "\n",
    "# Extraire les numéros de catégories\n",
    "y_cat_num = data['category_num'].values\n",
    "print(\"Numéros de catégories :\", y_cat_num)\n"
   ],
   "id": "7359fbc3d8522550",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catégories : ['[\"home furnishing', '[\"baby care', '[\"watches', '[\"home decor & festive needs', '[\"kitchen & dining', '[\"beauty and personal care', '[\"computers']\n",
      "Numéros de catégories : [0 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 5 : Préparation du traitement du texte",
   "id": "1ceb245273393688"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:36:56.908054Z",
     "start_time": "2024-09-14T07:36:56.897341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import des librairies pour le traitement du texte\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Téléchargement des ressources NLTK (à exécuter une seule fois)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ],
   "id": "89bce2fc53d85e85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:36:57.137889Z",
     "start_time": "2024-09-14T07:36:57.132329Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Check import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Exemple de texte\n",
    "texte = \"Bonjour, ceci est un test de tokenisation.\"\n",
    "\n",
    "# Tokenisation du texte\n",
    "tokens = word_tokenize(texte)\n",
    "\n",
    "# Affichage des tokens\n",
    "print(tokens)"
   ],
   "id": "ce97e913fc669517",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour', ',', 'ceci', 'est', 'un', 'test', 'de', 'tokenisation', '.']\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 6 : Définition des fonctions de prétraitement",
   "id": "dbd7a8be34d460a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:36:57.345755Z",
     "start_time": "2024-09-14T07:36:57.330358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fonction de tokenisation\n",
    "def tokenizer_fct(sentence):\n",
    "    sentence_clean = sentence.replace('-', ' ').replace('+', ' ').replace('/', ' ').replace('#', ' ')\n",
    "    word_tokens = word_tokenize(sentence_clean)\n",
    "    return word_tokens\n",
    "\n",
    "# Liste des stop words\n",
    "stop_w = set(stopwords.words('english'))\n",
    "punctuations = ['[', ']', ',', '.', ':', '?', '(', ')', '!', ';', '\"', \"'\", '&', '%', '$', '@', '#', '*', '–', '—', '...']\n",
    "stop_w.update(punctuations)\n",
    "\n",
    "# Fonction de filtrage des stop words\n",
    "def stop_word_filter_fct(list_words):\n",
    "    filtered_w = [w for w in list_words if not w.lower() in stop_w]\n",
    "    filtered_w2 = [w for w in filtered_w if len(w) > 2]\n",
    "    return filtered_w2\n",
    "\n",
    "# Fonction de mise en minuscule et suppression de certains mots\n",
    "def lower_start_fct(list_words):\n",
    "    lw = [w.lower() for w in list_words if not w.startswith(\"@\") and not w.startswith(\"#\") and not w.startswith(\"http\")]\n",
    "    return lw\n",
    "\n",
    "# Fonction de lemmatisation\n",
    "def lemma_fct(list_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "\n",
    "# Fonction de préparation du texte pour Bag-of-Words\n",
    "def transform_bow_fct(desc_text):\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour Bag-of-Words avec lemmatisation\n",
    "def transform_bow_lem_fct(desc_text):\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    lem_w = lemma_fct(lw)\n",
    "    transf_desc_text = ' '.join(lem_w)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour les modèles de Deep Learning (BERT, USE)\n",
    "def transform_dl_fct(desc_text):\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    lw = lower_start_fct(word_tokens)\n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n"
   ],
   "id": "2d45a28a6e4fe3cd",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 7 : Application du prétraitement",
   "id": "1207b7466a637a39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:37:00.827030Z",
     "start_time": "2024-09-14T07:36:57.385763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remplacer les valeurs manquantes dans 'description' par une chaîne vide\n",
    "data['description'] = data['description'].fillna('')\n",
    "\n",
    "# Application des fonctions de prétraitement\n",
    "data['sentence_bow'] = data['description'].apply(lambda x: transform_bow_fct(str(x)))\n",
    "data['sentence_bow_lem'] = data['description'].apply(lambda x: transform_bow_lem_fct(str(x)))\n",
    "data['sentence_dl'] = data['description'].apply(lambda x: transform_dl_fct(str(x)))\n",
    "\n",
    "print(\"Dimensions du dataset après prétraitement :\", data.shape)\n"
   ],
   "id": "3ca4949bbdbc058",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du dataset après prétraitement : (1050, 20)\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 8 : Sauvegarde des données prétraitées (optionnel)",
   "id": "ba4b0e597b53ef92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:37:00.965254Z",
     "start_time": "2024-09-14T07:37:00.873393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Définir le chemin relatif vers le dossier \"data\"\n",
    "file_path = os.path.join(\"..\", \"data\", \"preprocessed_product_data.csv\")\n",
    "\n",
    "# Sauvegarde du dataset prétraité pour une utilisation future\n",
    "data.to_csv(file_path, index=False)\n"
   ],
   "id": "ae55979fd98f9e69",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:37:01.036644Z",
     "start_time": "2024-09-14T07:37:01.029529Z"
    }
   },
   "cell_type": "code",
   "source": "print(data.columns)\n",
   "id": "1121c5a36e796a1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uniq_id', 'crawl_timestamp', 'product_url', 'product_name',\n",
      "       'product_category_tree', 'pid', 'retail_price', 'discounted_price',\n",
      "       'image', 'is_FK_Advantage_product', 'description', 'product_rating',\n",
      "       'overall_rating', 'brand', 'product_specifications', 'category',\n",
      "       'category_num', 'sentence_bow', 'sentence_bow_lem', 'sentence_dl'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 10 : Extraction des features avec Bag-of-Words et TF-IDF\n",
   "id": "20092ab904ed2103"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:37:01.223772Z",
     "start_time": "2024-09-14T07:37:01.090527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Téléchargement des ressources NLTK (à exécuter une seule fois)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialiser les outils de NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fonction pour nettoyer et lemmatiser les catégories de produits\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('[', '').replace(']', '')  # Supprimer les crochets\n",
    "    tokens = word_tokenize(text.lower())  # Conversion en minuscules et tokenisation\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]  # Lemmatisation et suppression des stop words\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Appliquer la fonction de prétraitement à la colonne 'category'\n",
    "data['sentence_bow_lem'] = data['category'].apply(preprocess_text)\n",
    "\n",
    "# Vérifier la colonne nouvellement créée\n",
    "print(data[['category', 'sentence_bow_lem']].head())\n"
   ],
   "id": "150a7e265c5ac37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            category sentence_bow_lem\n",
      "0  [\"home furnishing  home furnishing\n",
      "1        [\"baby care        baby care\n",
      "2        [\"baby care        baby care\n",
      "3  [\"home furnishing  home furnishing\n",
      "4  [\"home furnishing  home furnishing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:37:01.324132Z",
     "start_time": "2024-09-14T07:37:01.291686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialisation des vectoriseurs\n",
    "cvect = CountVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "\n",
    "# Sélection de la colonne à utiliser\n",
    "feat = 'sentence_bow_lem'\n",
    "\n",
    "# Ajustement et transformation des données\n",
    "cv_fit = cvect.fit(data[feat])\n",
    "tfidf_fit = tfidf_vect.fit(data[feat])\n",
    "\n",
    "cv_transform = cvect.transform(data[feat])\n",
    "tfidf_transform = tfidf_vect.transform(data[feat])\n"
   ],
   "id": "224aaa8c7d83f2cf",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 11 : Sauvegarde des embeddings Bag-of-Words",
   "id": "2a8063348cc2804a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:37:01.379248Z",
     "start_time": "2024-09-14T07:37:01.367147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Conversion en format dense (attention à la taille de la matrice)\n",
    "cv_features = cv_transform.toarray()\n",
    "\n",
    "# Vérification ou création de la colonne 'product_id'\n",
    "if 'uniq_id' in data.columns:\n",
    "    data.rename(columns={'uniq_id': 'product_id'}, inplace=True)\n",
    "elif 'product_id' not in data.columns:\n",
    "    data['product_id'] = data.index\n",
    "\n",
    "product_ids = data['product_id'].values\n",
    "\n",
    "# Sauvegarde des embeddings et des identifiants de produits\n",
    "np.savez_compressed('countvectorizer_embeddings.npz', product_id=product_ids, embeddings=cv_features)\n"
   ],
   "id": "e87824b8d0243447",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 12 : Sauvegarde des embeddings TF-IDF",
   "id": "6320d9065806c8c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:37:01.428453Z",
     "start_time": "2024-09-14T07:37:01.417381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Conversion en format dense\n",
    "tfidf_features = tfidf_transform.toarray()\n",
    "\n",
    "# Sauvegarde des embeddings et des identifiants de produits\n",
    "np.savez_compressed('tfidf_embeddings.npz', product_id=product_ids, embeddings=tfidf_features)\n"
   ],
   "id": "2c5fa542db437216",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 13 : Extraction des features avec Word2Vec",
   "id": "22a88c8eb517e3a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:37:02.152495Z",
     "start_time": "2024-09-14T07:37:01.479670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gensim\n",
    "\n",
    "# Préparation des phrases pour Word2Vec\n",
    "sentences = data['sentence_bow_lem'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Paramètres du modèle Word2Vec\n",
    "w2v_size = 300\n",
    "w2v_window = 5\n",
    "w2v_min_count = 1\n",
    "w2v_epochs = 100\n",
    "\n",
    "# Entraînement du modèle Word2Vec\n",
    "print(\"Entraînement du modèle Word2Vec...\")\n",
    "w2v_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=w2v_size,\n",
    "    window=w2v_window,\n",
    "    min_count=w2v_min_count,\n",
    "    workers=4,\n",
    "    seed=42,\n",
    "    epochs=w2v_epochs\n",
    ")\n",
    "\n",
    "# Fonction pour obtenir l'embedding d'un document\n",
    "def document_vector(doc):\n",
    "    doc = [word for word in doc if word in w2v_model.wv.key_to_index]\n",
    "    return np.mean(w2v_model.wv[doc], axis=0) if len(doc) > 0 else np.zeros(w2v_size)\n",
    "\n",
    "# Création des embeddings pour chaque description\n",
    "w2v_embeddings = np.array([document_vector(doc) for doc in sentences])\n",
    "\n",
    "# Sauvegarde des embeddings Word2Vec\n",
    "np.savez_compressed('word2vec_embeddings.npz', product_id=product_ids, embeddings=w2v_embeddings)\n"
   ],
   "id": "439a75c5c061665d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle Word2Vec...\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 14 : Extraction des features avec BERT\n",
   "id": "e83f1b40e6ac88f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:38:13.179763Z",
     "start_time": "2024-09-14T07:37:02.198584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements spécifiques\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning, module = \"transformers\")\n",
    "\n",
    "# Paramètres\n",
    "max_length = 64\n",
    "batch_size = 32\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Chargement du tokenizer et du modèle BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.clean_up_tokenization_spaces = True  # Optionnel, pour supprimer le FutureWarning\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Préparation des inputs\n",
    "sentences = data['sentence_dl'].tolist()\n",
    "\n",
    "\n",
    "def bert_encode(sentences, tokenizer, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded = tokenizer(\n",
    "            sentence,\n",
    "            add_special_tokens = True,\n",
    "            max_length = max_length,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'tf'\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return tf.concat(input_ids, axis = 0), tf.concat(attention_masks, axis = 0)\n",
    "\n",
    "\n",
    "input_ids, attention_masks = bert_encode(sentences, tokenizer, max_length)\n",
    "\n",
    "# Génération des embeddings\n",
    "embeddings = []\n",
    "num_examples = len(sentences)\n",
    "\n",
    "for i in range(0, num_examples, batch_size):\n",
    "    batch_input_ids = input_ids[i:i + batch_size]\n",
    "    batch_attention_masks = attention_masks[i:i + batch_size]\n",
    "    outputs = model(batch_input_ids, attention_mask = batch_attention_masks)\n",
    "    # Utilisation de la représentation du token [CLS]\n",
    "    batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "bert_embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Assurez-vous que 'product_ids' est défini\n",
    "# Par exemple:\n",
    "# product_ids = data['product_id'].tolist()\n",
    "\n",
    "# Sauvegarde des embeddings BERT\n",
    "np.savez_compressed('bert_embeddings.npz', product_id = product_ids, embeddings = bert_embeddings)\n"
   ],
   "id": "c2182fa03ca16c3a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:38:13.228408Z",
     "start_time": "2024-09-14T07:38:13.221995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from transformers import AutoTokenizer, TFAutoModel\n",
    "# import tensorflow as tf\n",
    "# \n",
    "# # Paramètres\n",
    "# max_length = 64\n",
    "# batch_size = 32\n",
    "# model_name = 'bert-base-uncased'\n",
    "# \n",
    "# # Chargement du tokenizer et du modèle BERT\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = TFAutoModel.from_pretrained(model_name)\n",
    "# \n",
    "# # Préparation des inputs\n",
    "# sentences = data['sentence_dl'].tolist()\n",
    "# \n",
    "# def bert_encode(sentences, tokenizer, max_length):\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "# \n",
    "#     for sentence in sentences:\n",
    "#         encoded = tokenizer.encode_plus(\n",
    "#             sentence,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=max_length,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             return_attention_mask=True,\n",
    "#             return_tensors='tf',            \n",
    "#         )\n",
    "#         input_ids.append(encoded['input_ids'])\n",
    "#         attention_masks.append(encoded['attention_mask'])\n",
    "#     return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
    "# \n",
    "# input_ids, attention_masks = bert_encode(sentences, tokenizer, max_length)\n",
    "# \n",
    "# # Génération des embeddings\n",
    "# embeddings = []\n",
    "# num_examples = len(sentences)\n",
    "# \n",
    "# for i in range(0, num_examples, batch_size):\n",
    "#     batch_input_ids = input_ids[i:i+batch_size]\n",
    "#     batch_attention_masks = attention_masks[i:i+batch_size]\n",
    "#     outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
    "#     # Utilisation de la représentation du token [CLS]\n",
    "#     batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "#     embeddings.append(batch_embeddings)\n",
    "# \n",
    "# bert_embeddings = np.vstack(embeddings)\n",
    "# \n",
    "# # Sauvegarde des embeddings BERT\n",
    "# np.savez_compressed('bert_embeddings.npz', product_id=product_ids, embeddings=bert_embeddings)\n"
   ],
   "id": "df9e0094e9e3dec4",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 15 : Extraction des features avec Universal Sentence Encoder (USE)",
   "id": "d540217fa331418"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:38:20.333587Z",
     "start_time": "2024-09-14T07:38:13.273502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Chargement du modèle USE\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Génération des embeddings\n",
    "sentences = data['sentence_dl'].tolist()\n",
    "batch_size = 32\n",
    "\n",
    "embeddings = []\n",
    "num_examples = len(sentences)\n",
    "\n",
    "for i in range(0, num_examples, batch_size):\n",
    "    batch_sentences = sentences[i:i+batch_size]\n",
    "    batch_embeddings = embed(batch_sentences).numpy()\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "use_embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Sauvegarde des embeddings USE\n",
    "np.savez_compressed('use_embeddings.npz', product_id=product_ids, embeddings=use_embeddings)"
   ],
   "id": "66a02362817e4c5b",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 16 : Sauvegarde des identifiants de produits et des catégories",
   "id": "219be7b002421f3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:38:20.376013Z",
     "start_time": "2024-09-14T07:38:20.370940Z"
    }
   },
   "cell_type": "code",
   "source": "print(data.columns)",
   "id": "63d82d21a892c619",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['product_id', 'crawl_timestamp', 'product_url', 'product_name',\n",
      "       'product_category_tree', 'pid', 'retail_price', 'discounted_price',\n",
      "       'image', 'is_FK_Advantage_product', 'description', 'product_rating',\n",
      "       'overall_rating', 'brand', 'product_specifications', 'category',\n",
      "       'category_num', 'sentence_bow', 'sentence_bow_lem', 'sentence_dl'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:38:20.489080Z",
     "start_time": "2024-09-14T07:38:20.478783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sauvegarde des identifiants de produits et des catégories\n",
    "product_info = data[['product_id', 'category', 'category_num']]\n",
    "file_path = os.path.join(\"..\", \"data\", \"product_info.csv\")\n",
    "product_info.to_csv(file_path, index=False)\n"
   ],
   "id": "e8384f83e55d65d1",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 17 : Vérification des embeddings générés (optionnel)",
   "id": "f13731cc75a0c007"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:38:20.547682Z",
     "start_time": "2024-09-14T07:38:20.541180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vérifier les dimensions des embeddings\n",
    "print(\"Dimensions des embeddings CountVectorizer :\", cv_features.shape)\n",
    "print(\"Dimensions des embeddings TF-IDF :\", tfidf_features.shape)\n",
    "print(\"Dimensions des embeddings Word2Vec :\", w2v_embeddings.shape)\n",
    "print(\"Dimensions des embeddings BERT :\", bert_embeddings.shape)\n",
    "print(\"Dimensions des embeddings USE :\", use_embeddings.shape)\n"
   ],
   "id": "61164210561c8259",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des embeddings CountVectorizer : (1050, 13)\n",
      "Dimensions des embeddings TF-IDF : (1050, 13)\n",
      "Dimensions des embeddings Word2Vec : (1050, 300)\n",
      "Dimensions des embeddings BERT : (1050, 768)\n",
      "Dimensions des embeddings USE : (1050, 512)\n"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Les résultats que nous obtenons montrent les dimensions des embeddings générés à partir de différentes méthodes pour une collection de 1050 documents ou échantillons. Voici mes observations :\n",
    "\n",
    "1. **CountVectorizer et TF-IDF** : Les dimensions sont `(1050, 13)` pour les deux méthodes. Cela signifie que le vocabulaire extrait des documents (ou caractéristiques) contient 13 termes uniques après la vectorisation. Ce nombre peut paraître faible et suggère que le texte contient peu de termes distincts ou que certaines options de prétraitement, comme l’élimination des mots très fréquents ou rares, ont réduit significativement le nombre de caractéristiques.\n",
    "   \n",
    "2. **Word2Vec** : Les dimensions sont `(1050, 300)`. Cela correspond à l'embedding standard de Word2Vec, où chaque document est représenté par une moyenne pondérée des vecteurs de mots (chaque mot étant encodé dans un espace de 300 dimensions). Ce nombre est constant pour chaque document, car il correspond à la dimension des vecteurs de mots appris par Word2Vec.\n",
    "\n",
    "3. **BERT** : Les dimensions sont `(1050, 768)`. BERT produit des vecteurs d’une dimension fixe de 768 pour chaque document. Il s'agit de l'une des caractéristiques du modèle pré-entraîné BERT (base), qui génère un vecteur d'embedding de 768 dimensions pour représenter chaque phrase ou document.\n",
    "\n",
    "4. **USE (Universal Sentence Encoder)** : Les dimensions sont `(1050, 512)`. De manière similaire à BERT, USE génère des vecteurs de taille fixe, ici 512 dimensions. Le modèle USE est spécifiquement conçu pour capturer les relations sémantiques au niveau des phrases, et la taille de ses embeddings est fixe.\n",
    "\n",
    "### Comparaison des dimensions :\n",
    "\n",
    "- **CountVectorizer et TF-IDF** offrent des représentations très basiques (13 dimensions), limitées par la taille du vocabulaire. Elles capturent peu d’informations sémantiques.\n",
    "- **Word2Vec, BERT et USE** fournissent des embeddings beaucoup plus riches avec respectivement 300, 768 et 512 dimensions, capturant mieux la sémantique des documents. Les modèles de type Word2Vec et BERT/USE sont préférables pour des tâches où les relations sémantiques sont importantes.\n",
    "\n",
    "En résumé, les embeddings de CountVectorizer et TF-IDF sont simples et basés uniquement sur les fréquences des mots, tandis que ceux de Word2Vec, BERT, et USE capturent des informations contextuelles et sémantiques plus riches."
   ],
   "id": "e7e16630dad771e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "98b2af45ad02bcf5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
