{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 1 : Importation des librairies essentielles python",
   "id": "837f7c652c46b58d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import des librairies essentielles\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 2 : Importation des données",
   "id": "e3d462b424487c9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Définir le chemin relatif\n",
    "file_path = os.path.join(\"..\", \"data\", \"flipkart_com-ecommerce_sample_1050.csv\")\n",
    "\n",
    "# Chargement du dataset des produits\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Dimensions du dataset :\", data.shape)\n"
   ],
   "id": "25500449b6b27523"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 3 : Affichage d'un aperçu des données",
   "id": "f8f1d994554b404a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Afficher les premières lignes du dataset\n",
    "data.head()"
   ],
   "id": "afad58cc0e5c8f5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 4 : Vérification des colonnes disponibles",
   "id": "a2d37e68f96172f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Afficher les noms des colonnes\n",
    "print(\"Colonnes du dataset :\", data.columns.tolist())\n"
   ],
   "id": "39474df71d1701f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 5 : Préparation du traitement du texte",
   "id": "1ceb245273393688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import des librairies pour le traitement du texte\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Téléchargement des ressources NLTK (à exécuter une seule fois)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ],
   "id": "89bce2fc53d85e85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 6 : Définition des fonctions de prétraitement",
   "id": "dbd7a8be34d460a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fonction de tokenisation\n",
    "def tokenizer_fct(sentence):\n",
    "    sentence_clean = sentence.replace('-', ' ').replace('+', ' ').replace('/', ' ').replace('#', ' ')\n",
    "    word_tokens = word_tokenize(sentence_clean)\n",
    "    return word_tokens\n",
    "\n",
    "# Liste des stop words\n",
    "stop_w = set(stopwords.words('english'))\n",
    "punctuations = ['[', ']', ',', '.', ':', '?', '(', ')', '!', ';', '\"', \"'\", '&', '%', '$', '@', '#', '*', '–', '—', '...']\n",
    "stop_w.update(punctuations)\n",
    "\n",
    "# Fonction de filtrage des stop words\n",
    "def stop_word_filter_fct(list_words):\n",
    "    filtered_w = [w for w in list_words if not w.lower() in stop_w]\n",
    "    filtered_w2 = [w for w in filtered_w if len(w) > 2]\n",
    "    return filtered_w2\n",
    "\n",
    "# Fonction de mise en minuscule et suppression de certains mots\n",
    "def lower_start_fct(list_words):\n",
    "    lw = [w.lower() for w in list_words if not w.startswith(\"@\") and not w.startswith(\"#\") and not w.startswith(\"http\")]\n",
    "    return lw\n",
    "\n",
    "# Fonction de lemmatisation\n",
    "def lemma_fct(list_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "\n",
    "# Fonction de préparation du texte pour Bag-of-Words\n",
    "def transform_bow_fct(desc_text):\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour Bag-of-Words avec lemmatisation\n",
    "def transform_bow_lem_fct(desc_text):\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    lem_w = lemma_fct(lw)\n",
    "    transf_desc_text = ' '.join(lem_w)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour les modèles de Deep Learning (BERT, USE)\n",
    "def transform_dl_fct(desc_text):\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    lw = lower_start_fct(word_tokens)\n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n"
   ],
   "id": "2d45a28a6e4fe3cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 7 : Application du prétraitement",
   "id": "1207b7466a637a39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remplacer les valeurs manquantes dans 'description' par une chaîne vide\n",
    "data['description'] = data['description'].fillna('')\n",
    "\n",
    "# Application des fonctions de prétraitement\n",
    "data['sentence_bow'] = data['description'].apply(lambda x: transform_bow_fct(str(x)))\n",
    "data['sentence_bow_lem'] = data['description'].apply(lambda x: transform_bow_lem_fct(str(x)))\n",
    "data['sentence_dl'] = data['description'].apply(lambda x: transform_dl_fct(str(x)))\n",
    "\n",
    "print(\"Dimensions du dataset après prétraitement :\", data.shape)\n"
   ],
   "id": "3ca4949bbdbc058"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 8 : Sauvegarde des données prétraitées (optionnel)",
   "id": "ba4b0e597b53ef92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sauvegarde du dataset prétraité pour une utilisation future\n",
    "data.to_csv('preprocessed_product_data.csv', index=False)\n"
   ],
   "id": "bf56b36a8dddec12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 9 : Préparation des catégories",
   "id": "cd5c3e89a4afb38d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remplacer les valeurs manquantes dans 'product_category' par 'Unknown'\n",
    "data['product_category'] = data['product_category'].fillna('Unknown')\n",
    "\n",
    "# Liste des catégories\n",
    "categories = data['product_category'].unique().tolist()\n",
    "print(\"Catégories :\", categories)\n",
    "\n",
    "# Mapping des catégories vers des numéros\n",
    "category_to_num = {category: idx for idx, category in enumerate(categories)}\n",
    "data['category_num'] = data['product_category'].map(category_to_num)\n",
    "y_cat_num = data['category_num'].values\n"
   ],
   "id": "315d6e1804a384f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 10 : Extraction des features avec Bag-of-Words et TF-IDF\n",
   "id": "20092ab904ed2103"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialisation des vectoriseurs\n",
    "cvect = CountVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "\n",
    "# Sélection de la colonne à utiliser\n",
    "feat = 'sentence_bow_lem'\n",
    "\n",
    "# Ajustement et transformation des données\n",
    "cv_fit = cvect.fit(data[feat])\n",
    "tfidf_fit = tfidf_vect.fit(data[feat])\n",
    "\n",
    "cv_transform = cvect.transform(data[feat])\n",
    "tfidf_transform = tfidf_vect.transform(data[feat])\n"
   ],
   "id": "224aaa8c7d83f2cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 11 : Sauvegarde des embeddings Bag-of-Words",
   "id": "2a8063348cc2804a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Conversion en format dense (attention à la taille de la matrice)\n",
    "cv_features = cv_transform.toarray()\n",
    "\n",
    "# Vérification ou création de la colonne 'product_id'\n",
    "if 'uniq_id' in data.columns:\n",
    "    data.rename(columns={'uniq_id': 'product_id'}, inplace=True)\n",
    "elif 'product_id' not in data.columns:\n",
    "    data['product_id'] = data.index\n",
    "\n",
    "product_ids = data['product_id'].values\n",
    "\n",
    "# Sauvegarde des embeddings et des identifiants de produits\n",
    "np.savez_compressed('countvectorizer_embeddings.npz', product_id=product_ids, embeddings=cv_features)\n"
   ],
   "id": "e87824b8d0243447"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 12 : Sauvegarde des embeddings TF-IDF",
   "id": "6320d9065806c8c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Conversion en format dense\n",
    "tfidf_features = tfidf_transform.toarray()\n",
    "\n",
    "# Sauvegarde des embeddings et des identifiants de produits\n",
    "np.savez_compressed('tfidf_embeddings.npz', product_id=product_ids, embeddings=tfidf_features)\n"
   ],
   "id": "2c5fa542db437216"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 13 : Extraction des features avec Word2Vec",
   "id": "22a88c8eb517e3a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gensim\n",
    "\n",
    "# Préparation des phrases pour Word2Vec\n",
    "sentences = data['sentence_bow_lem'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Paramètres du modèle Word2Vec\n",
    "w2v_size = 300\n",
    "w2v_window = 5\n",
    "w2v_min_count = 1\n",
    "w2v_epochs = 100\n",
    "\n",
    "# Entraînement du modèle Word2Vec\n",
    "print(\"Entraînement du modèle Word2Vec...\")\n",
    "w2v_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=w2v_size,\n",
    "    window=w2v_window,\n",
    "    min_count=w2v_min_count,\n",
    "    workers=4,\n",
    "    seed=42,\n",
    "    epochs=w2v_epochs\n",
    ")\n",
    "\n",
    "# Fonction pour obtenir l'embedding d'un document\n",
    "def document_vector(doc):\n",
    "    doc = [word for word in doc if word in w2v_model.wv.key_to_index]\n",
    "    return np.mean(w2v_model.wv[doc], axis=0) if len(doc) > 0 else np.zeros(w2v_size)\n",
    "\n",
    "# Création des embeddings pour chaque description\n",
    "w2v_embeddings = np.array([document_vector(doc) for doc in sentences])\n",
    "\n",
    "# Sauvegarde des embeddings Word2Vec\n",
    "np.savez_compressed('word2vec_embeddings.npz', product_id=product_ids, embeddings=w2v_embeddings)\n"
   ],
   "id": "439a75c5c061665d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 14 : Extraction des features avec BERT\n",
   "id": "e83f1b40e6ac88f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Paramètres\n",
    "max_length = 64\n",
    "batch_size = 32\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Chargement du tokenizer et du modèle BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Préparation des inputs\n",
    "sentences = data['sentence_dl'].tolist()\n",
    "\n",
    "def bert_encode(sentences, tokenizer, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf',\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
    "\n",
    "input_ids, attention_masks = bert_encode(sentences, tokenizer, max_length)\n",
    "\n",
    "# Génération des embeddings\n",
    "embeddings = []\n",
    "num_examples = len(sentences)\n",
    "\n",
    "for i in range(0, num_examples, batch_size):\n",
    "    batch_input_ids = input_ids[i:i+batch_size]\n",
    "    batch_attention_masks = attention_masks[i:i+batch_size]\n",
    "    outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
    "    # Utilisation de la représentation du token [CLS]\n",
    "    batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "bert_embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Sauvegarde des embeddings BERT\n",
    "np.savez_compressed('bert_embeddings.npz', product_id=product_ids, embeddings=bert_embeddings)\n"
   ],
   "id": "df9e0094e9e3dec4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 15 : Extraction des features avec Universal Sentence Encoder (USE)",
   "id": "d540217fa331418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Chargement du modèle USE\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Génération des embeddings\n",
    "sentences = data['sentence_dl'].tolist()\n",
    "batch_size = 32\n",
    "\n",
    "embeddings = []\n",
    "num_examples = len(sentences)\n",
    "\n",
    "for i in range(0, num_examples, batch_size):\n",
    "    batch_sentences = sentences[i:i+batch_size]\n",
    "    batch_embeddings = embed(batch_sentences).numpy()\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "use_embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Sauvegarde des embeddings USE\n",
    "np.savez_compressed('use_embeddings.npz', product_id=product_ids, embeddings=use_embeddings)\n"
   ],
   "id": "13f1fcb7b06f5edb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 16 : Sauvegarde des identifiants de produits et des catégories",
   "id": "219be7b002421f3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sauvegarde des identifiants de produits et des catégories\n",
    "product_info = data[['product_id', 'product_category', 'category_num']]\n",
    "product_info.to_csv('product_info.csv', index=False)\n"
   ],
   "id": "e8384f83e55d65d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 17 : Vérification des embeddings générés (optionnel)",
   "id": "f13731cc75a0c007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Vérifier les dimensions des embeddings\n",
    "print(\"Dimensions des embeddings CountVectorizer :\", cv_features.shape)\n",
    "print(\"Dimensions des embeddings TF-IDF :\", tfidf_features.shape)\n",
    "print(\"Dimensions des embeddings Word2Vec :\", w2v_embeddings.shape)\n",
    "print(\"Dimensions des embeddings BERT :\", bert_embeddings.shape)\n",
    "print(\"Dimensions des embeddings USE :\", use_embeddings.shape)\n"
   ],
   "id": "61164210561c8259"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
