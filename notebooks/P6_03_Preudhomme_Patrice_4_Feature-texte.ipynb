{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 1 : Importation des librairies essentielles python",
   "id": "837f7c652c46b58d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-13T10:29:01.034665Z",
     "start_time": "2024-09-13T10:29:01.028968Z"
    }
   },
   "source": [
    "# Import des librairies essentielles\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 2 : Importation des données",
   "id": "e3d462b424487c9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:29:03.125168Z",
     "start_time": "2024-09-13T10:29:03.087020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Définir le chemin relatif\n",
    "file_path = os.path.join(\"..\", \"data\", \"flipkart_com-ecommerce_sample_1050.csv\")\n",
    "\n",
    "# Chargement du dataset des produits\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Dimensions du dataset :\", data.shape)\n"
   ],
   "id": "25500449b6b27523",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du dataset : (1050, 15)\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 3 : Affichage d'un aperçu des données",
   "id": "f8f1d994554b404a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:29:07.088041Z",
     "start_time": "2024-09-13T10:29:07.071628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Afficher les premières lignes du dataset\n",
    "data.head()"
   ],
   "id": "afad58cc0e5c8f5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                            uniq_id            crawl_timestamp  \\\n",
       "0  55b85ea15a1536d46b7190ad6fff8ce7  2016-04-30 03:22:56 +0000   \n",
       "1  7b72c92c2f6c40268628ec5f14c6d590  2016-04-30 03:22:56 +0000   \n",
       "2  64d5d4a258243731dc7bbb1eef49ad74  2016-04-30 03:22:56 +0000   \n",
       "3  d4684dcdc759dd9cdf41504698d737d8  2016-06-20 08:49:52 +0000   \n",
       "4  6325b6870c54cd47be6ebfbffa620ec7  2016-06-20 08:49:52 +0000   \n",
       "\n",
       "                                         product_url  \\\n",
       "0  http://www.flipkart.com/elegance-polyester-mul...   \n",
       "1  http://www.flipkart.com/sathiyas-cotton-bath-t...   \n",
       "2  http://www.flipkart.com/eurospa-cotton-terry-f...   \n",
       "3  http://www.flipkart.com/santosh-royal-fashion-...   \n",
       "4  http://www.flipkart.com/jaipur-print-cotton-fl...   \n",
       "\n",
       "                                        product_name  \\\n",
       "0  Elegance Polyester Multicolor Abstract Eyelet ...   \n",
       "1                         Sathiyas Cotton Bath Towel   \n",
       "2                Eurospa Cotton Terry Face Towel Set   \n",
       "3  SANTOSH ROYAL FASHION Cotton Printed King size...   \n",
       "4  Jaipur Print Cotton Floral King sized Double B...   \n",
       "\n",
       "                               product_category_tree               pid  \\\n",
       "0  [\"Home Furnishing >> Curtains & Accessories >>...  CRNEG7BKMFFYHQ8Z   \n",
       "1  [\"Baby Care >> Baby Bath & Skin >> Baby Bath T...  BTWEGFZHGBXPHZUH   \n",
       "2  [\"Baby Care >> Baby Bath & Skin >> Baby Bath T...  BTWEG6SHXTDB2A2Y   \n",
       "3  [\"Home Furnishing >> Bed Linen >> Bedsheets >>...  BDSEJT9UQWHDUBH4   \n",
       "4  [\"Home Furnishing >> Bed Linen >> Bedsheets >>...  BDSEJTHNGWVGWWQU   \n",
       "\n",
       "   retail_price  discounted_price                                 image  \\\n",
       "0        1899.0             899.0  55b85ea15a1536d46b7190ad6fff8ce7.jpg   \n",
       "1         600.0             449.0  7b72c92c2f6c40268628ec5f14c6d590.jpg   \n",
       "2           NaN               NaN  64d5d4a258243731dc7bbb1eef49ad74.jpg   \n",
       "3        2699.0            1299.0  d4684dcdc759dd9cdf41504698d737d8.jpg   \n",
       "4        2599.0             698.0  6325b6870c54cd47be6ebfbffa620ec7.jpg   \n",
       "\n",
       "   is_FK_Advantage_product                                        description  \\\n",
       "0                    False  Key Features of Elegance Polyester Multicolor ...   \n",
       "1                    False  Specifications of Sathiyas Cotton Bath Towel (...   \n",
       "2                    False  Key Features of Eurospa Cotton Terry Face Towe...   \n",
       "3                    False  Key Features of SANTOSH ROYAL FASHION Cotton P...   \n",
       "4                    False  Key Features of Jaipur Print Cotton Floral Kin...   \n",
       "\n",
       "        product_rating       overall_rating                  brand  \\\n",
       "0  No rating available  No rating available               Elegance   \n",
       "1  No rating available  No rating available               Sathiyas   \n",
       "2  No rating available  No rating available                Eurospa   \n",
       "3  No rating available  No rating available  SANTOSH ROYAL FASHION   \n",
       "4  No rating available  No rating available           Jaipur Print   \n",
       "\n",
       "                              product_specifications  \n",
       "0  {\"product_specification\"=>[{\"key\"=>\"Brand\", \"v...  \n",
       "1  {\"product_specification\"=>[{\"key\"=>\"Machine Wa...  \n",
       "2  {\"product_specification\"=>[{\"key\"=>\"Material\",...  \n",
       "3  {\"product_specification\"=>[{\"key\"=>\"Brand\", \"v...  \n",
       "4  {\"product_specification\"=>[{\"key\"=>\"Machine Wa...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>crawl_timestamp</th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_category_tree</th>\n",
       "      <th>pid</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>discounted_price</th>\n",
       "      <th>image</th>\n",
       "      <th>is_FK_Advantage_product</th>\n",
       "      <th>description</th>\n",
       "      <th>product_rating</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_specifications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7</td>\n",
       "      <td>2016-04-30 03:22:56 +0000</td>\n",
       "      <td>http://www.flipkart.com/elegance-polyester-mul...</td>\n",
       "      <td>Elegance Polyester Multicolor Abstract Eyelet ...</td>\n",
       "      <td>[\"Home Furnishing &gt;&gt; Curtains &amp; Accessories &gt;&gt;...</td>\n",
       "      <td>CRNEG7BKMFFYHQ8Z</td>\n",
       "      <td>1899.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Key Features of Elegance Polyester Multicolor ...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Elegance</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Brand\", \"v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590</td>\n",
       "      <td>2016-04-30 03:22:56 +0000</td>\n",
       "      <td>http://www.flipkart.com/sathiyas-cotton-bath-t...</td>\n",
       "      <td>Sathiyas Cotton Bath Towel</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby Bath &amp; Skin &gt;&gt; Baby Bath T...</td>\n",
       "      <td>BTWEGFZHGBXPHZUH</td>\n",
       "      <td>600.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Specifications of Sathiyas Cotton Bath Towel (...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Sathiyas</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Machine Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74</td>\n",
       "      <td>2016-04-30 03:22:56 +0000</td>\n",
       "      <td>http://www.flipkart.com/eurospa-cotton-terry-f...</td>\n",
       "      <td>Eurospa Cotton Terry Face Towel Set</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby Bath &amp; Skin &gt;&gt; Baby Bath T...</td>\n",
       "      <td>BTWEG6SHXTDB2A2Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Key Features of Eurospa Cotton Terry Face Towe...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Eurospa</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Material\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d4684dcdc759dd9cdf41504698d737d8</td>\n",
       "      <td>2016-06-20 08:49:52 +0000</td>\n",
       "      <td>http://www.flipkart.com/santosh-royal-fashion-...</td>\n",
       "      <td>SANTOSH ROYAL FASHION Cotton Printed King size...</td>\n",
       "      <td>[\"Home Furnishing &gt;&gt; Bed Linen &gt;&gt; Bedsheets &gt;&gt;...</td>\n",
       "      <td>BDSEJT9UQWHDUBH4</td>\n",
       "      <td>2699.0</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>d4684dcdc759dd9cdf41504698d737d8.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Key Features of SANTOSH ROYAL FASHION Cotton P...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>SANTOSH ROYAL FASHION</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Brand\", \"v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6325b6870c54cd47be6ebfbffa620ec7</td>\n",
       "      <td>2016-06-20 08:49:52 +0000</td>\n",
       "      <td>http://www.flipkart.com/jaipur-print-cotton-fl...</td>\n",
       "      <td>Jaipur Print Cotton Floral King sized Double B...</td>\n",
       "      <td>[\"Home Furnishing &gt;&gt; Bed Linen &gt;&gt; Bedsheets &gt;&gt;...</td>\n",
       "      <td>BDSEJTHNGWVGWWQU</td>\n",
       "      <td>2599.0</td>\n",
       "      <td>698.0</td>\n",
       "      <td>6325b6870c54cd47be6ebfbffa620ec7.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Key Features of Jaipur Print Cotton Floral Kin...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Jaipur Print</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Machine Wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 4 : Vérification des colonnes disponibles",
   "id": "a2d37e68f96172f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:29:10.179056Z",
     "start_time": "2024-09-13T10:29:10.174813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Afficher les noms des colonnes\n",
    "print(\"Colonnes du dataset :\", data.columns.tolist())\n"
   ],
   "id": "39474df71d1701f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes du dataset : ['uniq_id', 'crawl_timestamp', 'product_url', 'product_name', 'product_category_tree', 'pid', 'retail_price', 'discounted_price', 'image', 'is_FK_Advantage_product', 'description', 'product_rating', 'overall_rating', 'brand', 'product_specifications']\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 9 : Préparation des catégories",
   "id": "cd5c3e89a4afb38d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:29:39.886389Z",
     "start_time": "2024-09-13T10:29:39.871621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Extraire la première catégorie de la colonne 'product_category_tree'\n",
    "data['category'] = data['product_category_tree'].apply(lambda x: x.split('>>')[0].strip())\n",
    "\n",
    "# Mettre les catégories en minuscules pour uniformiser\n",
    "data['category'] = data['category'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remplacer les valeurs manquantes dans 'category' par 'unknown'\n",
    "data['category'] = data['category'].fillna('unknown')\n",
    "\n",
    "# Liste des catégories uniques\n",
    "categories = data['category'].unique().tolist()\n",
    "print(\"Catégories :\", categories)\n",
    "\n",
    "# Mapping des catégories vers des numéros\n",
    "category_to_num = {category: idx for idx, category in enumerate(categories)}\n",
    "data['category_num'] = data['category'].map(category_to_num)\n",
    "\n",
    "# Extraire les numéros de catégories\n",
    "y_cat_num = data['category_num'].values\n",
    "print(\"Numéros de catégories :\", y_cat_num)\n"
   ],
   "id": "7359fbc3d8522550",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catégories : ['[\"home furnishing', '[\"baby care', '[\"watches', '[\"home decor & festive needs', '[\"kitchen & dining', '[\"beauty and personal care', '[\"computers']\n",
      "Numéros de catégories : [0 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 5 : Préparation du traitement du texte",
   "id": "1ceb245273393688"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:29:47.336277Z",
     "start_time": "2024-09-13T10:29:47.327267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import des librairies pour le traitement du texte\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Téléchargement des ressources NLTK (à exécuter une seule fois)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ],
   "id": "89bce2fc53d85e85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:29:50.780402Z",
     "start_time": "2024-09-13T10:29:50.774343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Exemple de texte\n",
    "texte = \"Bonjour, ceci est un test de tokenisation.\"\n",
    "\n",
    "# Tokenisation du texte\n",
    "tokens = word_tokenize(texte)\n",
    "\n",
    "# Affichage des tokens\n",
    "print(tokens)"
   ],
   "id": "ce97e913fc669517",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour', ',', 'ceci', 'est', 'un', 'test', 'de', 'tokenisation', '.']\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 6 : Définition des fonctions de prétraitement",
   "id": "dbd7a8be34d460a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:29:55.700381Z",
     "start_time": "2024-09-13T10:29:55.688800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fonction de tokenisation\n",
    "def tokenizer_fct(sentence):\n",
    "    sentence_clean = sentence.replace('-', ' ').replace('+', ' ').replace('/', ' ').replace('#', ' ')\n",
    "    word_tokens = word_tokenize(sentence_clean)\n",
    "    return word_tokens\n",
    "\n",
    "# Liste des stop words\n",
    "stop_w = set(stopwords.words('english'))\n",
    "punctuations = ['[', ']', ',', '.', ':', '?', '(', ')', '!', ';', '\"', \"'\", '&', '%', '$', '@', '#', '*', '–', '—', '...']\n",
    "stop_w.update(punctuations)\n",
    "\n",
    "# Fonction de filtrage des stop words\n",
    "def stop_word_filter_fct(list_words):\n",
    "    filtered_w = [w for w in list_words if not w.lower() in stop_w]\n",
    "    filtered_w2 = [w for w in filtered_w if len(w) > 2]\n",
    "    return filtered_w2\n",
    "\n",
    "# Fonction de mise en minuscule et suppression de certains mots\n",
    "def lower_start_fct(list_words):\n",
    "    lw = [w.lower() for w in list_words if not w.startswith(\"@\") and not w.startswith(\"#\") and not w.startswith(\"http\")]\n",
    "    return lw\n",
    "\n",
    "# Fonction de lemmatisation\n",
    "def lemma_fct(list_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "\n",
    "# Fonction de préparation du texte pour Bag-of-Words\n",
    "def transform_bow_fct(desc_text):\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour Bag-of-Words avec lemmatisation\n",
    "def transform_bow_lem_fct(desc_text):\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    lem_w = lemma_fct(lw)\n",
    "    transf_desc_text = ' '.join(lem_w)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour les modèles de Deep Learning (BERT, USE)\n",
    "def transform_dl_fct(desc_text):\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    lw = lower_start_fct(word_tokens)\n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n"
   ],
   "id": "2d45a28a6e4fe3cd",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 7 : Application du prétraitement",
   "id": "1207b7466a637a39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:30:09.583745Z",
     "start_time": "2024-09-13T10:30:07.161427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remplacer les valeurs manquantes dans 'description' par une chaîne vide\n",
    "data['description'] = data['description'].fillna('')\n",
    "\n",
    "# Application des fonctions de prétraitement\n",
    "data['sentence_bow'] = data['description'].apply(lambda x: transform_bow_fct(str(x)))\n",
    "data['sentence_bow_lem'] = data['description'].apply(lambda x: transform_bow_lem_fct(str(x)))\n",
    "data['sentence_dl'] = data['description'].apply(lambda x: transform_dl_fct(str(x)))\n",
    "\n",
    "print(\"Dimensions du dataset après prétraitement :\", data.shape)\n"
   ],
   "id": "3ca4949bbdbc058",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du dataset après prétraitement : (1050, 20)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 8 : Sauvegarde des données prétraitées (optionnel)",
   "id": "ba4b0e597b53ef92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:30:17.102647Z",
     "start_time": "2024-09-13T10:30:17.032873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Définir le chemin relatif vers le dossier \"data\"\n",
    "file_path = os.path.join(\"..\", \"data\", \"preprocessed_product_data.csv\")\n",
    "\n",
    "# Sauvegarde du dataset prétraité pour une utilisation future\n",
    "data.to_csv(file_path, index=False)\n"
   ],
   "id": "ae55979fd98f9e69",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:32:01.081976Z",
     "start_time": "2024-09-13T10:32:01.077303Z"
    }
   },
   "cell_type": "code",
   "source": "print(data.columns)\n",
   "id": "1121c5a36e796a1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uniq_id', 'crawl_timestamp', 'product_url', 'product_name',\n",
      "       'product_category_tree', 'pid', 'retail_price', 'discounted_price',\n",
      "       'image', 'is_FK_Advantage_product', 'description', 'product_rating',\n",
      "       'overall_rating', 'brand', 'product_specifications', 'category',\n",
      "       'category_num', 'sentence_bow', 'sentence_bow_lem', 'sentence_dl'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 10 : Extraction des features avec Bag-of-Words et TF-IDF\n",
   "id": "20092ab904ed2103"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:35:04.803714Z",
     "start_time": "2024-09-13T10:35:04.672306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Téléchargement des ressources NLTK (à exécuter une seule fois)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialiser les outils de NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fonction pour nettoyer et lemmatiser les catégories de produits\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('[', '').replace(']', '')  # Supprimer les crochets\n",
    "    tokens = word_tokenize(text.lower())  # Conversion en minuscules et tokenisation\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]  # Lemmatisation et suppression des stop words\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Appliquer la fonction de prétraitement à la colonne 'category'\n",
    "data['sentence_bow_lem'] = data['category'].apply(preprocess_text)\n",
    "\n",
    "# Vérifier la colonne nouvellement créée\n",
    "print(data[['category', 'sentence_bow_lem']].head())\n"
   ],
   "id": "150a7e265c5ac37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            category sentence_bow_lem\n",
      "0  [\"home furnishing  home furnishing\n",
      "1        [\"baby care        baby care\n",
      "2        [\"baby care        baby care\n",
      "3  [\"home furnishing  home furnishing\n",
      "4  [\"home furnishing  home furnishing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:35:39.294795Z",
     "start_time": "2024-09-13T10:35:39.263233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialisation des vectoriseurs\n",
    "cvect = CountVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "\n",
    "# Sélection de la colonne à utiliser\n",
    "feat = 'sentence_bow_lem'\n",
    "\n",
    "# Ajustement et transformation des données\n",
    "cv_fit = cvect.fit(data[feat])\n",
    "tfidf_fit = tfidf_vect.fit(data[feat])\n",
    "\n",
    "cv_transform = cvect.transform(data[feat])\n",
    "tfidf_transform = tfidf_vect.transform(data[feat])\n"
   ],
   "id": "224aaa8c7d83f2cf",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 11 : Sauvegarde des embeddings Bag-of-Words",
   "id": "2a8063348cc2804a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:35:44.658086Z",
     "start_time": "2024-09-13T10:35:44.646441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Conversion en format dense (attention à la taille de la matrice)\n",
    "cv_features = cv_transform.toarray()\n",
    "\n",
    "# Vérification ou création de la colonne 'product_id'\n",
    "if 'uniq_id' in data.columns:\n",
    "    data.rename(columns={'uniq_id': 'product_id'}, inplace=True)\n",
    "elif 'product_id' not in data.columns:\n",
    "    data['product_id'] = data.index\n",
    "\n",
    "product_ids = data['product_id'].values\n",
    "\n",
    "# Sauvegarde des embeddings et des identifiants de produits\n",
    "np.savez_compressed('countvectorizer_embeddings.npz', product_id=product_ids, embeddings=cv_features)\n"
   ],
   "id": "e87824b8d0243447",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 12 : Sauvegarde des embeddings TF-IDF",
   "id": "6320d9065806c8c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:35:50.263387Z",
     "start_time": "2024-09-13T10:35:50.252818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Conversion en format dense\n",
    "tfidf_features = tfidf_transform.toarray()\n",
    "\n",
    "# Sauvegarde des embeddings et des identifiants de produits\n",
    "np.savez_compressed('tfidf_embeddings.npz', product_id=product_ids, embeddings=tfidf_features)\n"
   ],
   "id": "2c5fa542db437216",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 13 : Extraction des features avec Word2Vec",
   "id": "22a88c8eb517e3a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:42:16.834166Z",
     "start_time": "2024-09-13T10:42:15.807251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gensim\n",
    "\n",
    "# Préparation des phrases pour Word2Vec\n",
    "sentences = data['sentence_bow_lem'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Paramètres du modèle Word2Vec\n",
    "w2v_size = 300\n",
    "w2v_window = 5\n",
    "w2v_min_count = 1\n",
    "w2v_epochs = 100\n",
    "\n",
    "# Entraînement du modèle Word2Vec\n",
    "print(\"Entraînement du modèle Word2Vec...\")\n",
    "w2v_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=w2v_size,\n",
    "    window=w2v_window,\n",
    "    min_count=w2v_min_count,\n",
    "    workers=4,\n",
    "    seed=42,\n",
    "    epochs=w2v_epochs\n",
    ")\n",
    "\n",
    "# Fonction pour obtenir l'embedding d'un document\n",
    "def document_vector(doc):\n",
    "    doc = [word for word in doc if word in w2v_model.wv.key_to_index]\n",
    "    return np.mean(w2v_model.wv[doc], axis=0) if len(doc) > 0 else np.zeros(w2v_size)\n",
    "\n",
    "# Création des embeddings pour chaque description\n",
    "w2v_embeddings = np.array([document_vector(doc) for doc in sentences])\n",
    "\n",
    "# Sauvegarde des embeddings Word2Vec\n",
    "np.savez_compressed('word2vec_embeddings.npz', product_id=product_ids, embeddings=w2v_embeddings)\n"
   ],
   "id": "439a75c5c061665d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle Word2Vec...\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 14 : Extraction des features avec BERT\n",
   "id": "e83f1b40e6ac88f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T11:24:51.472509Z",
     "start_time": "2024-09-13T11:24:50.394201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Paramètres\n",
    "max_length = 64\n",
    "batch_size = 32\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Chargement du tokenizer et du modèle BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Préparation des inputs\n",
    "sentences = data['sentence_dl'].tolist()\n",
    "\n",
    "def bert_encode(sentences, tokenizer, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf',\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
    "\n",
    "input_ids, attention_masks = bert_encode(sentences, tokenizer, max_length)\n",
    "\n",
    "# Génération des embeddings\n",
    "embeddings = []\n",
    "num_examples = len(sentences)\n",
    "\n",
    "for i in range(0, num_examples, batch_size):\n",
    "    batch_input_ids = input_ids[i:i+batch_size]\n",
    "    batch_attention_masks = attention_masks[i:i+batch_size]\n",
    "    outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
    "    # Utilisation de la représentation du token [CLS]\n",
    "    batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "bert_embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Sauvegarde des embeddings BERT\n",
    "np.savez_compressed('bert_embeddings.npz', product_id=product_ids, embeddings=bert_embeddings)\n"
   ],
   "id": "df9e0094e9e3dec4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pat\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.engine'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1602\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1603\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1604\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1204\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1176\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1147\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:690\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:940\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:38\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_tf_outputs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     28\u001B[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001B[0;32m     29\u001B[0m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     36\u001B[0m     TFTokenClassifierOutput,\n\u001B[0;32m     37\u001B[0m )\n\u001B[1;32m---> 38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_tf_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     39\u001B[0m     TFCausalLanguageModelingLoss,\n\u001B[0;32m     40\u001B[0m     TFMaskedLanguageModelingLoss,\n\u001B[0;32m     41\u001B[0m     TFModelInputType,\n\u001B[0;32m     42\u001B[0m     TFMultipleChoiceLoss,\n\u001B[0;32m     43\u001B[0m     TFNextSentencePredictionLoss,\n\u001B[0;32m     44\u001B[0m     TFPreTrainedModel,\n\u001B[0;32m     45\u001B[0m     TFQuestionAnsweringLoss,\n\u001B[0;32m     46\u001B[0m     TFSequenceClassificationLoss,\n\u001B[0;32m     47\u001B[0m     TFTokenClassificationLoss,\n\u001B[0;32m     48\u001B[0m     get_initializer,\n\u001B[0;32m     49\u001B[0m     keras_serializable,\n\u001B[0;32m     50\u001B[0m     unpack_inputs,\n\u001B[0;32m     51\u001B[0m )\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtf_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m shape_list, stable_softmax\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:70\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend \u001B[38;5;28;01mas\u001B[39;00m K\n\u001B[1;32m---> 70\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data_adapter\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras_tensor\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KerasTensor\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'keras.engine'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[73], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Chargement du tokenizer et du modèle BERT\u001B[39;00m\n\u001B[0;32m     10\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[1;32m---> 11\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mTFAutoModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Préparation des inputs\u001B[39;00m\n\u001B[0;32m     14\u001B[0m sentences \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence_dl\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    559\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    560\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    561\u001B[0m     )\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m--> 563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m \u001B[43m_get_model_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model_mapping\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    565\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    566\u001B[0m     )\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    570\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:384\u001B[0m, in \u001B[0;36m_get_model_class\u001B[1;34m(config, model_mapping)\u001B[0m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_model_class\u001B[39m(config, model_mapping):\n\u001B[1;32m--> 384\u001B[0m     supported_models \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_mapping\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    385\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(supported_models, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m    386\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m supported_models\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:735\u001B[0m, in \u001B[0;36m_LazyAutoMapping.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    733\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_type \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping:\n\u001B[0;32m    734\u001B[0m     model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping[model_type]\n\u001B[1;32m--> 735\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_attr_from_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    737\u001B[0m \u001B[38;5;66;03m# Maybe there was several model types associated with this config.\u001B[39;00m\n\u001B[0;32m    738\u001B[0m model_types \u001B[38;5;241m=\u001B[39m [k \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config_mapping\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;241m==\u001B[39m key\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m]\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:749\u001B[0m, in \u001B[0;36m_LazyAutoMapping._load_attr_from_module\u001B[1;34m(self, model_type, attr)\u001B[0m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m module_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules[module_name] \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransformers.models\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 749\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgetattribute_from_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_modules\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:693\u001B[0m, in \u001B[0;36mgetattribute_from_module\u001B[1;34m(module, attr)\u001B[0m\n\u001B[0;32m    691\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(attr, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(getattribute_from_module(module, a) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m attr)\n\u001B[1;32m--> 693\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(module, attr):\n\u001B[0;32m    694\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, attr)\n\u001B[0;32m    695\u001B[0m \u001B[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001B[39;00m\n\u001B[0;32m    696\u001B[0m \u001B[38;5;66;03m# object at the top level.\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1591\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(name)\n\u001B[0;32m   1592\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m-> 1593\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1594\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m   1595\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1603\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m   1604\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1605\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1606\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1607\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1608\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.engine'"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 15 : Extraction des features avec Universal Sentence Encoder (USE)",
   "id": "d540217fa331418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Chargement du modèle USE\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Génération des embeddings\n",
    "sentences = data['sentence_dl'].tolist()\n",
    "batch_size = 32\n",
    "\n",
    "embeddings = []\n",
    "num_examples = len(sentences)\n",
    "\n",
    "for i in range(0, num_examples, batch_size):\n",
    "    batch_sentences = sentences[i:i+batch_size]\n",
    "    batch_embeddings = embed(batch_sentences).numpy()\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "use_embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Sauvegarde des embeddings USE\n",
    "np.savez_compressed('use_embeddings.npz', product_id=product_ids, embeddings=use_embeddings)\n"
   ],
   "id": "13f1fcb7b06f5edb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 16 : Sauvegarde des identifiants de produits et des catégories",
   "id": "219be7b002421f3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sauvegarde des identifiants de produits et des catégories\n",
    "product_info = data[['product_id', 'product_category', 'category_num']]\n",
    "product_info.to_csv('product_info.csv', index=False)\n"
   ],
   "id": "e8384f83e55d65d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cellule 17 : Vérification des embeddings générés (optionnel)",
   "id": "f13731cc75a0c007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Vérifier les dimensions des embeddings\n",
    "print(\"Dimensions des embeddings CountVectorizer :\", cv_features.shape)\n",
    "print(\"Dimensions des embeddings TF-IDF :\", tfidf_features.shape)\n",
    "print(\"Dimensions des embeddings Word2Vec :\", w2v_embeddings.shape)\n",
    "print(\"Dimensions des embeddings BERT :\", bert_embeddings.shape)\n",
    "print(\"Dimensions des embeddings USE :\", use_embeddings.shape)\n"
   ],
   "id": "61164210561c8259"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
