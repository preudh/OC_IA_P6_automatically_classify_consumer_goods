{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:47:54.077335Z",
     "start_time": "2024-09-21T18:47:53.988190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Charger les embeddings textuels (TF-IDF)\n",
    "text_embedding_file = os.path.join(\"..\", \"data\", \"tfidf_embeddings.npz\")\n",
    "text_data = np.load(text_embedding_file, allow_pickle=True)  # Charger avec allow_pickle=True\n",
    "text_embeddings = text_data['embeddings']\n",
    "product_ids_text = text_data['product_id']\n",
    "\n",
    "# Afficher les dimensions des embeddings textuels\n",
    "print(f\"Dimensions des embeddings textuels : {text_embeddings.shape}\")\n"
   ],
   "id": "57107ff54381eb2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des embeddings textuels : (1050, 5317)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:48:20.686623Z",
     "start_time": "2024-09-21T18:48:20.632361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Charger les données des vraies catégories\n",
    "file_path = os.path.join(\"..\", \"data\", \"preprocessed_product_data.csv\")\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Vérifier les premières lignes et la colonne des vraies catégories\n",
    "print(data.head())\n",
    "print(f\"Colonnes disponibles : {data.columns}\")\n"
   ],
   "id": "4a9a959510cb7432",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            uniq_id            crawl_timestamp  \\\n",
      "0  55b85ea15a1536d46b7190ad6fff8ce7  2016-04-30 03:22:56 +0000   \n",
      "1  7b72c92c2f6c40268628ec5f14c6d590  2016-04-30 03:22:56 +0000   \n",
      "2  64d5d4a258243731dc7bbb1eef49ad74  2016-04-30 03:22:56 +0000   \n",
      "3  d4684dcdc759dd9cdf41504698d737d8  2016-06-20 08:49:52 +0000   \n",
      "4  6325b6870c54cd47be6ebfbffa620ec7  2016-06-20 08:49:52 +0000   \n",
      "\n",
      "                                         product_url  \\\n",
      "0  http://www.flipkart.com/elegance-polyester-mul...   \n",
      "1  http://www.flipkart.com/sathiyas-cotton-bath-t...   \n",
      "2  http://www.flipkart.com/eurospa-cotton-terry-f...   \n",
      "3  http://www.flipkart.com/santosh-royal-fashion-...   \n",
      "4  http://www.flipkart.com/jaipur-print-cotton-fl...   \n",
      "\n",
      "                                        product_name  \\\n",
      "0  Elegance Polyester Multicolor Abstract Eyelet ...   \n",
      "1                         Sathiyas Cotton Bath Towel   \n",
      "2                Eurospa Cotton Terry Face Towel Set   \n",
      "3  SANTOSH ROYAL FASHION Cotton Printed King size...   \n",
      "4  Jaipur Print Cotton Floral King sized Double B...   \n",
      "\n",
      "                               product_category_tree               pid  \\\n",
      "0  [\"Home Furnishing >> Curtains & Accessories >>...  CRNEG7BKMFFYHQ8Z   \n",
      "1  [\"Baby Care >> Baby Bath & Skin >> Baby Bath T...  BTWEGFZHGBXPHZUH   \n",
      "2  [\"Baby Care >> Baby Bath & Skin >> Baby Bath T...  BTWEG6SHXTDB2A2Y   \n",
      "3  [\"Home Furnishing >> Bed Linen >> Bedsheets >>...  BDSEJT9UQWHDUBH4   \n",
      "4  [\"Home Furnishing >> Bed Linen >> Bedsheets >>...  BDSEJTHNGWVGWWQU   \n",
      "\n",
      "   retail_price  discounted_price                                 image  \\\n",
      "0        1899.0             899.0  55b85ea15a1536d46b7190ad6fff8ce7.jpg   \n",
      "1         600.0             449.0  7b72c92c2f6c40268628ec5f14c6d590.jpg   \n",
      "2           NaN               NaN  64d5d4a258243731dc7bbb1eef49ad74.jpg   \n",
      "3        2699.0            1299.0  d4684dcdc759dd9cdf41504698d737d8.jpg   \n",
      "4        2599.0             698.0  6325b6870c54cd47be6ebfbffa620ec7.jpg   \n",
      "\n",
      "   is_FK_Advantage_product                                        description  \\\n",
      "0                    False  Key Features of Elegance Polyester Multicolor ...   \n",
      "1                    False  Specifications of Sathiyas Cotton Bath Towel (...   \n",
      "2                    False  Key Features of Eurospa Cotton Terry Face Towe...   \n",
      "3                    False  Key Features of SANTOSH ROYAL FASHION Cotton P...   \n",
      "4                    False  Key Features of Jaipur Print Cotton Floral Kin...   \n",
      "\n",
      "        product_rating       overall_rating                  brand  \\\n",
      "0  No rating available  No rating available               Elegance   \n",
      "1  No rating available  No rating available               Sathiyas   \n",
      "2  No rating available  No rating available                Eurospa   \n",
      "3  No rating available  No rating available  SANTOSH ROYAL FASHION   \n",
      "4  No rating available  No rating available           Jaipur Print   \n",
      "\n",
      "                              product_specifications         category  \\\n",
      "0  {\"product_specification\"=>[{\"key\"=>\"Brand\", \"v...  home furnishing   \n",
      "1  {\"product_specification\"=>[{\"key\"=>\"Machine Wa...        baby care   \n",
      "2  {\"product_specification\"=>[{\"key\"=>\"Material\",...        baby care   \n",
      "3  {\"product_specification\"=>[{\"key\"=>\"Brand\", \"v...  home furnishing   \n",
      "4  {\"product_specification\"=>[{\"key\"=>\"Machine Wa...  home furnishing   \n",
      "\n",
      "   category_num                                       sentence_bow  \\\n",
      "0             0  key features elegance polyester multicolor abs...   \n",
      "1             1  specifications sathiyas cotton bath towel bath...   \n",
      "2             1  key features eurospa cotton terry face towel s...   \n",
      "3             0  key features santosh royal fashion cotton prin...   \n",
      "4             0  key features jaipur print cotton floral king s...   \n",
      "\n",
      "                                    sentence_bow_lem  \\\n",
      "0  key feature elegance polyester multicolor abst...   \n",
      "1  specification sathiyas cotton bath towel bath ...   \n",
      "2  key feature eurospa cotton terry face towel se...   \n",
      "3  key feature santosh royal fashion cotton print...   \n",
      "4  key feature jaipur print cotton floral king si...   \n",
      "\n",
      "                                         sentence_dl  \n",
      "0  key features of elegance polyester multicolor ...  \n",
      "1  specifications of sathiyas cotton bath towel (...  \n",
      "2  key features of eurospa cotton terry face towe...  \n",
      "3  key features of santosh royal fashion cotton p...  \n",
      "4  key features of jaipur print cotton floral kin...  \n",
      "Colonnes disponibles : Index(['uniq_id', 'crawl_timestamp', 'product_url', 'product_name',\n",
      "       'product_category_tree', 'pid', 'retail_price', 'discounted_price',\n",
      "       'image', 'is_FK_Advantage_product', 'description', 'product_rating',\n",
      "       'overall_rating', 'brand', 'product_specifications', 'category',\n",
      "       'category_num', 'sentence_bow', 'sentence_bow_lem', 'sentence_dl'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:48:03.883791Z",
     "start_time": "2024-09-21T18:48:03.875441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Charger les embeddings visuels (VGG16)\n",
    "visual_embedding_file = os.path.join(\"..\", \"models\", \"best_model2_vgg16_embeddings.npz\")\n",
    "visual_data = np.load(visual_embedding_file, allow_pickle=True)  # Charger avec allow_pickle=True\n",
    "visual_embeddings = visual_data['embeddings']\n",
    "product_ids_visual = visual_data['product_id']\n",
    "\n",
    "# Afficher les dimensions des embeddings visuels\n",
    "print(f\"Dimensions des embeddings visuels : {visual_embeddings.shape}\")\n"
   ],
   "id": "cd860537e2d9ff22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des embeddings visuels : (210, 7)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:49:17.849559Z",
     "start_time": "2024-09-21T18:49:17.840849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vérification des identifiants communs\n",
    "common_ids = np.intersect1d(product_ids_text, product_ids_visual.astype(str))\n",
    "print(f\"Nombre d'identifiants communs : {len(common_ids)}\")\n",
    "\n",
    "# Afficher quelques identifiants communs pour vérification\n",
    "print(f\"Premiers identifiants communs : {common_ids[:5]}\")\n"
   ],
   "id": "ffe2b28b81ce7fbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'identifiants communs : 0\n",
      "Premiers identifiants communs : []\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:49:34.834722Z",
     "start_time": "2024-09-21T18:49:34.828040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filtrer les embeddings textuels et visuels en fonction des identifiants communs\n",
    "text_embeddings_filtered = text_embeddings[np.isin(product_ids_text, common_ids)]\n",
    "visual_embeddings_filtered = visual_embeddings[np.isin(product_ids_visual.astype(str), common_ids)]\n",
    "\n",
    "# Vérifier les dimensions après filtrage\n",
    "print(f\"Dimensions des embeddings textuels filtrés : {text_embeddings_filtered.shape}\")\n",
    "print(f\"Dimensions des embeddings visuels filtrés : {visual_embeddings_filtered.shape}\")\n"
   ],
   "id": "e06d8cea5e15407a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des embeddings textuels filtrés : (0, 5317)\n",
      "Dimensions des embeddings visuels filtrés : (0, 7)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:49:54.273252Z",
     "start_time": "2024-09-21T18:49:54.266661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Concaténer les embeddings textuels et visuels\n",
    "combined_embeddings = np.concatenate([text_embeddings_filtered, visual_embeddings_filtered], axis=1)\n",
    "\n",
    "# Vérifier les dimensions des embeddings combinés\n",
    "print(f\"Dimensions des embeddings combinés : {combined_embeddings.shape}\")\n"
   ],
   "id": "d85e14aa0eacb7d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des embeddings combinés : (0, 5324)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:50:07.017514Z",
     "start_time": "2024-09-21T18:50:06.909305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "# Nombre de clusters basé sur le nombre de catégories réelles\n",
    "num_clusters = len(np.unique(data['category_num']))\n",
    "print(f\"Nombre de clusters : {num_clusters}\")\n",
    "\n",
    "# Appliquer KMeans\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(combined_embeddings)\n",
    "\n",
    "# Labels prédits par KMeans\n",
    "predicted_labels = kmeans.labels_\n",
    "\n",
    "# Vrais labels (catégories réelles)\n",
    "true_labels = data['category_num'].values\n"
   ],
   "id": "24359288abec8a13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de clusters : 7\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 5324)) while a minimum of 1 is required by KMeans.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Appliquer KMeans\u001B[39;00m\n\u001B[0;32m      9\u001B[0m kmeans \u001B[38;5;241m=\u001B[39m KMeans(n_clusters\u001B[38;5;241m=\u001B[39mnum_clusters, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m \u001B[43mkmeans\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcombined_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Labels prédits par KMeans\u001B[39;00m\n\u001B[0;32m     13\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m kmeans\u001B[38;5;241m.\u001B[39mlabels_\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1464\u001B[0m, in \u001B[0;36mKMeans.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1436\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   1437\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1438\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute k-means clustering.\u001B[39;00m\n\u001B[0;32m   1439\u001B[0m \n\u001B[0;32m   1440\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1462\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m   1463\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1464\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1465\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1466\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1468\u001B[0m \u001B[43m        \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1469\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1470\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1471\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1473\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params_vs_input(X)\n\u001B[0;32m   1475\u001B[0m     random_state \u001B[38;5;241m=\u001B[39m check_random_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\base.py:633\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    631\u001B[0m         out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 633\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[0;32m    635\u001B[0m     out \u001B[38;5;241m=\u001B[39m _check_y(y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\utils\\validation.py:1087\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m   1085\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_samples \u001B[38;5;241m<\u001B[39m ensure_min_samples:\n\u001B[1;32m-> 1087\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1088\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) while a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1089\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1090\u001B[0m             \u001B[38;5;241m%\u001B[39m (n_samples, array\u001B[38;5;241m.\u001B[39mshape, ensure_min_samples, context)\n\u001B[0;32m   1091\u001B[0m         )\n\u001B[0;32m   1093\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_features \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m   1094\u001B[0m     n_features \u001B[38;5;241m=\u001B[39m array\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: Found array with 0 sample(s) (shape=(0, 5324)) while a minimum of 1 is required by KMeans."
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:50:22.043309Z",
     "start_time": "2024-09-21T18:50:22.016472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculer le score ARI (Adjusted Rand Index)\n",
    "ari_score = metrics.adjusted_rand_score(true_labels, predicted_labels)\n",
    "print(f\"ARI Score for combined text + image embeddings: {ari_score:.4f}\")\n"
   ],
   "id": "60e19405dfbc9682",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Calculer le score ARI (Adjusted Rand Index)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m ari_score \u001B[38;5;241m=\u001B[39m metrics\u001B[38;5;241m.\u001B[39madjusted_rand_score(\u001B[43mtrue_labels\u001B[49m, predicted_labels)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mARI Score for combined text + image embeddings: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mari_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'true_labels' is not defined"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:50:31.801468Z",
     "start_time": "2024-09-21T18:50:31.792906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Enregistrer les embeddings concaténés\n",
    "output_file_combined = os.path.join(\"..\", \"data\", \"combined_text_image_embeddings.npz\")\n",
    "np.savez_compressed(output_file_combined, product_id=common_ids, embeddings=combined_embeddings)\n",
    "\n",
    "print(f\"Combined embeddings have been saved to: {output_file_combined}\")\n"
   ],
   "id": "7d6f16d33695b6f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined embeddings have been saved to: ..\\data\\combined_text_image_embeddings.npz\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:41:07.159231Z",
     "start_time": "2024-09-21T18:41:07.148068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Charger le fichier npz\n",
    "file_path = \"../models/best_model2_vgg16_embeddings.npz\"  # Modifiez le chemin si nécessaire\n",
    "data = np.load(file_path)\n",
    "\n",
    "# Afficher les clés disponibles dans le fichier\n",
    "print(\"Clés disponibles dans le fichier .npz :\", data.files)\n",
    "\n",
    "# Visualiser les premières lignes des embeddings et des identifiants\n",
    "product_ids = data['product_id']\n",
    "embeddings = data['embeddings']\n",
    "\n",
    "# Affichage des premières lignes\n",
    "print(f\"Premiers identifiants de produits : {product_ids[:5]}\")\n",
    "print(f\"Premiers embeddings (dimensions) : {embeddings[:5]}\")\n"
   ],
   "id": "f45bcd9c75299fbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clés disponibles dans le fichier .npz : ['product_id', 'embeddings']\n",
      "Premiers identifiants de produits : [0 1 2 3 4]\n",
      "Premiers embeddings (dimensions) : [[2.6755673e-01 7.6281163e-03 6.6825360e-01 1.0395801e-03 4.7152596e-08\n",
      "  5.5521332e-02 6.4392117e-07]\n",
      " [2.1867720e-12 6.2299409e-11 4.1947531e-22 1.0000000e+00 1.0719335e-11\n",
      "  1.5008108e-16 1.3913483e-14]\n",
      " [1.9252884e-14 1.7274643e-21 4.4928514e-22 9.4807976e-21 2.8298977e-24\n",
      "  9.1848936e-22 1.0000000e+00]\n",
      " [3.0350892e-04 4.7364594e-08 8.0512708e-09 9.9969482e-01 1.5984339e-08\n",
      "  1.6652419e-06 4.6188102e-13]\n",
      " [8.4365986e-04 5.2883530e-01 4.0030654e-09 4.5807463e-01 7.8298029e-10\n",
      "  1.2206303e-02 4.0079325e-05]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:45:14.746715Z",
     "start_time": "2024-09-21T18:45:14.740569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vérification des identifiants communs\n",
    "common_ids = np.intersect1d(product_ids_text, product_ids_visual.astype(str))\n",
    "print(f\"Nombre d'identifiants communs : {len(common_ids)}\")\n"
   ],
   "id": "8b6a2f4686c9583a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'identifiants communs : 0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:43:53.172257Z",
     "start_time": "2024-09-21T18:43:52.932891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Charger les données pour les vraies catégories\n",
    "file_path = os.path.join(\"..\", \"data\", \"preprocessed_product_data.csv\")\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Charger les embeddings textuels (TF-IDF)\n",
    "text_embedding_file = os.path.join(\"..\", \"data\", \"tfidf_embeddings.npz\")\n",
    "text_data = np.load(text_embedding_file, allow_pickle=True)  # Charger avec allow_pickle=True\n",
    "text_embeddings = text_data['embeddings']\n",
    "product_ids_text = text_data['product_id']\n",
    "\n",
    "# Charger les embeddings visuels (VGG16 - modèle 2)\n",
    "visual_embedding_file = os.path.join(\"..\", \"models\", \"best_model2_vgg16_embeddings.npz\")\n",
    "visual_data = np.load(visual_embedding_file, allow_pickle=True)  # Charger avec allow_pickle=True\n",
    "visual_embeddings = visual_data['embeddings']\n",
    "product_ids_visual = visual_data['product_id']\n",
    "\n",
    "# Solution 2: Réaligner les identifiants de produits\n",
    "common_ids = np.intersect1d(product_ids_text, product_ids_visual.astype(str))  # Identifiants communs\n",
    "\n",
    "# Filtrer les embeddings textuels et visuels en fonction des identifiants communs\n",
    "text_embeddings_filtered = text_embeddings[np.isin(product_ids_text, common_ids)]\n",
    "visual_embeddings_filtered = visual_embeddings[np.isin(product_ids_visual.astype(str), common_ids)]\n",
    "\n",
    "# Concaténer les embeddings textuels et visuels\n",
    "combined_embeddings = np.concatenate([text_embeddings_filtered, visual_embeddings_filtered], axis=1)\n",
    "\n",
    "# Appliquer KMeans sur les embeddings combinés\n",
    "num_clusters = len(np.unique(data['category_num']))  # Nombre de clusters selon les catégories réelles\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(combined_embeddings)\n",
    "\n",
    "# Labels prédits par KMeans\n",
    "predicted_labels = kmeans.labels_\n",
    "\n",
    "# Vrais labels (catégories réelles)\n",
    "true_labels = data['category_num'].values  # Les vraies catégories doivent être dans votre dataframe\n",
    "\n",
    "# Calculer le score ARI (Adjusted Rand Index)\n",
    "ari_score = metrics.adjusted_rand_score(true_labels, predicted_labels)\n",
    "print(f\"ARI Score for combined text + image embeddings: {ari_score:.4f}\")\n",
    "\n",
    "# Enregistrer les embeddings concaténés\n",
    "output_file_combined = os.path.join(\"..\", \"data\", \"combined_text_image_embeddings.npz\")\n",
    "np.savez_compressed(output_file_combined, product_id=common_ids, embeddings=combined_embeddings)\n",
    "print(f\"Combined embeddings have been saved to: {output_file_combined}\")\n"
   ],
   "id": "aff96fcba60a7deb",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 5324)) while a minimum of 1 is required by KMeans.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 36\u001B[0m\n\u001B[0;32m     34\u001B[0m num_clusters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory_num\u001B[39m\u001B[38;5;124m'\u001B[39m]))  \u001B[38;5;66;03m# Nombre de clusters selon les catégories réelles\u001B[39;00m\n\u001B[0;32m     35\u001B[0m kmeans \u001B[38;5;241m=\u001B[39m KMeans(n_clusters\u001B[38;5;241m=\u001B[39mnum_clusters, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m---> 36\u001B[0m \u001B[43mkmeans\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcombined_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Labels prédits par KMeans\u001B[39;00m\n\u001B[0;32m     39\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m kmeans\u001B[38;5;241m.\u001B[39mlabels_\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1464\u001B[0m, in \u001B[0;36mKMeans.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1436\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   1437\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1438\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute k-means clustering.\u001B[39;00m\n\u001B[0;32m   1439\u001B[0m \n\u001B[0;32m   1440\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1462\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m   1463\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1464\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1465\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1466\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1468\u001B[0m \u001B[43m        \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1469\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1470\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1471\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1473\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params_vs_input(X)\n\u001B[0;32m   1475\u001B[0m     random_state \u001B[38;5;241m=\u001B[39m check_random_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\base.py:633\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    631\u001B[0m         out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 633\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[0;32m    635\u001B[0m     out \u001B[38;5;241m=\u001B[39m _check_y(y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\utils\\validation.py:1087\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m   1085\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_samples \u001B[38;5;241m<\u001B[39m ensure_min_samples:\n\u001B[1;32m-> 1087\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1088\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) while a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1089\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1090\u001B[0m             \u001B[38;5;241m%\u001B[39m (n_samples, array\u001B[38;5;241m.\u001B[39mshape, ensure_min_samples, context)\n\u001B[0;32m   1091\u001B[0m         )\n\u001B[0;32m   1093\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_features \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m   1094\u001B[0m     n_features \u001B[38;5;241m=\u001B[39m array\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: Found array with 0 sample(s) (shape=(0, 5324)) while a minimum of 1 is required by KMeans."
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:34:29.161798Z",
     "start_time": "2024-09-21T18:34:28.148087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Charger les données pour les vraies catégories\n",
    "file_path = os.path.join(\"..\", \"data\", \"preprocessed_product_data.csv\")\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Charger les embeddings textuels (TF-IDF)\n",
    "text_embedding_file = os.path.join(\"..\", \"data\", \"tfidf_embeddings.npz\")\n",
    "text_data = np.load(text_embedding_file, allow_pickle=True)  # Charger avec allow_pickle=True\n",
    "text_embeddings = text_data['embeddings']\n",
    "product_ids_text = text_data['product_id']\n",
    "\n",
    "# Charger les embeddings visuels (VGG16 - modèle 2)\n",
    "visual_embedding_file = os.path.join(\"..\", \"models\", \"best_model2_vgg16_embeddings.npz\")\n",
    "visual_data = np.load(visual_embedding_file, allow_pickle=True)  # Charger avec allow_pickle=True\n",
    "visual_embeddings = visual_data['embeddings']\n",
    "product_ids_visual = visual_data['product_id']\n",
    "\n",
    "# Solution 2: Réaligner les identifiants de produits\n",
    "common_ids = np.intersect1d(product_ids_text, product_ids_visual.astype(str))  # Identifiants communs\n",
    "\n",
    "# Filtrer les embeddings textuels et visuels en fonction des identifiants communs\n",
    "text_embeddings_filtered = text_embeddings[np.isin(product_ids_text, common_ids)]\n",
    "visual_embeddings_filtered = visual_embeddings[np.isin(product_ids_visual.astype(str), common_ids)]\n",
    "\n",
    "# Concaténer les embeddings textuels et visuels\n",
    "combined_embeddings = np.concatenate([text_embeddings_filtered, visual_embeddings_filtered], axis=1)\n",
    "\n",
    "# Appliquer KMeans sur les embeddings combinés\n",
    "num_clusters = len(np.unique(data['category_num']))  # Nombre de clusters selon les catégories réelles\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(combined_embeddings)\n",
    "\n",
    "# Labels prédits par KMeans\n",
    "predicted_labels = kmeans.labels_\n",
    "\n",
    "# Vrais labels (catégories réelles)\n",
    "true_labels = data['category_num'].values  # Les vraies catégories doivent être dans votre dataframe\n",
    "\n",
    "# Calculer le score ARI (Adjusted Rand Index)\n",
    "ari_score = metrics.adjusted_rand_score(true_labels, predicted_labels)\n",
    "print(f\"ARI Score for combined text + image embeddings: {ari_score:.4f}\")\n",
    "\n",
    "# Enregistrer les embeddings concaténés\n",
    "output_file_combined = os.path.join(\"..\", \"data\", \"combined_text_image_embeddings.npz\")\n",
    "np.savez_compressed(output_file_combined, product_id=common_ids, embeddings=combined_embeddings)\n",
    "print(f\"Combined embeddings have been saved to: {output_file_combined}\")\n"
   ],
   "id": "1ced0b6df71f0915",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 5324)) while a minimum of 1 is required by KMeans.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 36\u001B[0m\n\u001B[0;32m     34\u001B[0m num_clusters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory_num\u001B[39m\u001B[38;5;124m'\u001B[39m]))  \u001B[38;5;66;03m# Nombre de clusters selon les catégories réelles\u001B[39;00m\n\u001B[0;32m     35\u001B[0m kmeans \u001B[38;5;241m=\u001B[39m KMeans(n_clusters\u001B[38;5;241m=\u001B[39mnum_clusters, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m---> 36\u001B[0m \u001B[43mkmeans\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcombined_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Labels prédits par KMeans\u001B[39;00m\n\u001B[0;32m     39\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m kmeans\u001B[38;5;241m.\u001B[39mlabels_\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1464\u001B[0m, in \u001B[0;36mKMeans.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1436\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   1437\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1438\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute k-means clustering.\u001B[39;00m\n\u001B[0;32m   1439\u001B[0m \n\u001B[0;32m   1440\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1462\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m   1463\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1464\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1465\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1466\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1468\u001B[0m \u001B[43m        \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1469\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1470\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1471\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1473\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params_vs_input(X)\n\u001B[0;32m   1475\u001B[0m     random_state \u001B[38;5;241m=\u001B[39m check_random_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\base.py:633\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    631\u001B[0m         out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 633\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[0;32m    635\u001B[0m     out \u001B[38;5;241m=\u001B[39m _check_y(y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\sklearn\\utils\\validation.py:1087\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m   1085\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_samples \u001B[38;5;241m<\u001B[39m ensure_min_samples:\n\u001B[1;32m-> 1087\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1088\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) while a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1089\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1090\u001B[0m             \u001B[38;5;241m%\u001B[39m (n_samples, array\u001B[38;5;241m.\u001B[39mshape, ensure_min_samples, context)\n\u001B[0;32m   1091\u001B[0m         )\n\u001B[0;32m   1093\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_features \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m   1094\u001B[0m     n_features \u001B[38;5;241m=\u001B[39m array\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: Found array with 0 sample(s) (shape=(0, 5324)) while a minimum of 1 is required by KMeans."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:35:54.768878Z",
     "start_time": "2024-09-21T18:35:54.761252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "common_ids = np.intersect1d(product_ids_text, product_ids_visual.astype(str))\n",
    "print(f\"Nombre d'identifiants communs : {len(common_ids)}\")\n"
   ],
   "id": "19baef6a2c864de8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'identifiants communs : 0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:36:14.180603Z",
     "start_time": "2024-09-21T18:36:14.173837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_embeddings_filtered = text_embeddings[np.isin(product_ids_text, common_ids)]\n",
    "visual_embeddings_filtered = visual_embeddings[np.isin(product_ids_visual.astype(str), common_ids)]\n",
    "\n",
    "print(f\"Dimensions des embeddings textuels filtrés : {text_embeddings_filtered.shape}\")\n",
    "print(f\"Dimensions des embeddings visuels filtrés : {visual_embeddings_filtered.shape}\")\n"
   ],
   "id": "7f39763ac1ef87ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des embeddings textuels filtrés : (0, 5317)\n",
      "Dimensions des embeddings visuels filtrés : (0, 7)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:37:49.028080Z",
     "start_time": "2024-09-21T18:37:49.023360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Premiers identifiants textuels : {product_ids_text[:5]}\")\n",
    "print(f\"Premiers identifiants visuels : {product_ids_visual[:5]}\")\n"
   ],
   "id": "cfb7d2b5407ea831",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premiers identifiants textuels : ['55b85ea15a1536d46b7190ad6fff8ce7' '7b72c92c2f6c40268628ec5f14c6d590'\n",
      " '64d5d4a258243731dc7bbb1eef49ad74' 'd4684dcdc759dd9cdf41504698d737d8'\n",
      " '6325b6870c54cd47be6ebfbffa620ec7']\n",
      "Premiers identifiants visuels : [0 1 2 3 4]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:28:46.507377Z",
     "start_time": "2024-09-21T18:28:46.320483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Charger les données pour les vraies catégories\n",
    "file_path = os.path.join(\"..\", \"data\", \"preprocessed_product_data.csv\")\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Charger les embeddings textuels (TF-IDF)\n",
    "text_embedding_file = os.path.join(\"..\", \"data\", \"tfidf_embeddings.npz\")\n",
    "text_data = np.load(text_embedding_file, allow_pickle=True)  # Charger avec allow_pickle=True\n",
    "text_embeddings = text_data['embeddings']\n",
    "product_ids_text = text_data['product_id']\n",
    "\n",
    "# Charger les embeddings visuels (VGG16 - modèle 2)\n",
    "visual_embedding_file = os.path.join(\"..\", \"models\", \"best_model2_vgg16_embeddings.npz\")\n",
    "visual_data = np.load(visual_embedding_file, allow_pickle=True)  # Charger avec allow_pickle=True\n",
    "visual_embeddings = visual_data['embeddings']\n",
    "product_ids_visual = visual_data['product_id']\n",
    "\n",
    "# Vérifier que les identifiants de produits sont dans le même ordre (ou les réaligner)\n",
    "assert np.array_equal(product_ids_text, product_ids_visual), \"Les identifiants de produits ne correspondent pas entre les deux embeddings !\"\n",
    "\n",
    "# Concaténer les embeddings textuels et visuels\n",
    "combined_embeddings = np.concatenate([text_embeddings, visual_embeddings], axis=1)\n",
    "\n",
    "# Appliquer KMeans sur les embeddings combinés\n",
    "num_clusters = len(np.unique(data['category_num']))  # Nombre de clusters selon les catégories réelles\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(combined_embeddings)\n",
    "\n",
    "# Labels prédits par KMeans\n",
    "predicted_labels = kmeans.labels_\n",
    "\n",
    "# Vrais labels (catégories réelles)\n",
    "true_labels = data['category_num'].values  # Les vraies catégories doivent être dans votre dataframe\n",
    "\n",
    "# Calculer le score ARI (Adjusted Rand Index)\n",
    "ari_score = metrics.adjusted_rand_score(true_labels, predicted_labels)\n",
    "print(f\"ARI Score for combined text + image embeddings: {ari_score:.4f}\")\n",
    "\n",
    "# Enregistrer les embeddings concaténés\n",
    "output_file_combined = os.path.join(\"..\", \"data\", \"combined_text_image_embeddings.npz\")\n",
    "np.savez_compressed(output_file_combined, product_id=product_ids_text, embeddings=combined_embeddings)\n",
    "print(f\"Combined embeddings have been saved to: {output_file_combined}\")\n"
   ],
   "id": "69a2d44ffa6ea910",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Les identifiants de produits ne correspondent pas entre les deux embeddings !",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 24\u001B[0m\n\u001B[0;32m     21\u001B[0m product_ids_visual \u001B[38;5;241m=\u001B[39m visual_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Vérifier que les identifiants de produits sont dans le même ordre (ou les réaligner)\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray_equal(product_ids_text, product_ids_visual), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLes identifiants de produits ne correspondent pas entre les deux embeddings !\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# Concaténer les embeddings textuels et visuels\u001B[39;00m\n\u001B[0;32m     27\u001B[0m combined_embeddings \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([text_embeddings, visual_embeddings], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mAssertionError\u001B[0m: Les identifiants de produits ne correspondent pas entre les deux embeddings !"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:31:10.170985Z",
     "start_time": "2024-09-21T18:31:10.166399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Product IDs (Text):\", product_ids_text)\n",
    "print(\"Product IDs (Visual):\", product_ids_visual)\n"
   ],
   "id": "bd6f6810c56df363",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product IDs (Text): ['55b85ea15a1536d46b7190ad6fff8ce7' '7b72c92c2f6c40268628ec5f14c6d590'\n",
      " '64d5d4a258243731dc7bbb1eef49ad74' ... '5912e037d12774bb73a2048f35a00009'\n",
      " 'c3edc504d1b4f0ba6224fa53a43a7ad6' 'f2f027ad6a6df617c9f125173da71e44']\n",
      "Product IDs (Visual): [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T18:25:43.660845Z",
     "start_time": "2024-09-21T18:25:42.290719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn import metrics\n",
    "# import pandas as pd\n",
    "# \n",
    "# # Charger les données pour les vraies catégories\n",
    "# file_path = os.path.join(\"..\", \"data\", \"preprocessed_product_data.csv\")\n",
    "# data = pd.read_csv(file_path)\n",
    "# \n",
    "# # Charger les embeddings textuels (TF-IDF)\n",
    "# text_embedding_file = os.path.join(\"..\", \"data\", \"tfidf_embeddings.npz\")\n",
    "# text_data = np.load(text_embedding_file)\n",
    "# text_embeddings = text_data['embeddings']\n",
    "# product_ids_text = text_data['product_id']\n",
    "# \n",
    "# # Charger les embeddings visuels (VGG16 - modèle 2)\n",
    "# visual_embedding_file = os.path.join(\"..\", \"models\", \"best_model2_vgg16_embeddings.npz\")  # Chemin vers les embeddings VGG16 modèle 2\n",
    "# visual_data = np.load(visual_embedding_file)\n",
    "# visual_embeddings = visual_data['embeddings']\n",
    "# product_ids_visual = visual_data['product_id']\n",
    "# \n",
    "# # Vérifier que les identifiants de produits sont dans le même ordre (ou les réaligner)\n",
    "# assert np.array_equal(product_ids_text, product_ids_visual), \"Les identifiants de produits ne correspondent pas entre les deux embeddings !\"\n",
    "# \n",
    "# # Concaténer les embeddings textuels et visuels\n",
    "# combined_embeddings = np.concatenate([text_embeddings, visual_embeddings], axis=1)\n",
    "# \n",
    "# # Appliquer KMeans sur les embeddings combinés\n",
    "# num_clusters = len(np.unique(data['category_num']))  # Nombre de clusters selon les catégories réelles\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "# kmeans.fit(combined_embeddings)\n",
    "# \n",
    "# # Labels prédits par KMeans\n",
    "# predicted_labels = kmeans.labels_\n",
    "# \n",
    "# # Vrais labels (catégories réelles)\n",
    "# true_labels = data['category_num'].values  # Les vraies catégories doivent être dans votre dataframe\n",
    "# \n",
    "# # Calculer le score ARI (Adjusted Rand Index)\n",
    "# ari_score = metrics.adjusted_rand_score(true_labels, predicted_labels)\n",
    "# print(f\"ARI Score for combined text + image embeddings: {ari_score:.4f}\")\n",
    "# \n",
    "# # Enregistrer les embeddings concaténés\n",
    "# output_file_combined = os.path.join(\"..\", \"data\", \"combined_text_image_embeddings.npz\")\n",
    "# np.savez_compressed(output_file_combined, product_id=product_ids_text, embeddings=combined_embeddings)\n",
    "# print(f\"Combined embeddings have been saved to: {output_file_combined}\")\n"
   ],
   "id": "44ec611998468636",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m text_data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(text_embedding_file)\n\u001B[0;32m     14\u001B[0m text_embeddings \u001B[38;5;241m=\u001B[39m text_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membeddings\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 15\u001B[0m product_ids_text \u001B[38;5;241m=\u001B[39m \u001B[43mtext_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mproduct_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Charger les embeddings visuels (VGG16 - modèle 2)\u001B[39;00m\n\u001B[0;32m     18\u001B[0m visual_embedding_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest_model2_vgg16_embeddings.npz\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Chemin vers les embeddings VGG16 modèle 2\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\numpy\\lib\\npyio.py:256\u001B[0m, in \u001B[0;36mNpzFile.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m magic \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m.\u001B[39mMAGIC_PREFIX:\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;28mbytes\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mzip\u001B[38;5;241m.\u001B[39mopen(key)\n\u001B[1;32m--> 256\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_array\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mbytes\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mallow_pickle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mallow_pickle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mpickle_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpickle_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mmax_header_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_header_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mzip\u001B[38;5;241m.\u001B[39mread(key)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P6q\\Lib\\site-packages\\numpy\\lib\\format.py:795\u001B[0m, in \u001B[0;36mread_array\u001B[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001B[0m\n\u001B[0;32m    792\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype\u001B[38;5;241m.\u001B[39mhasobject:\n\u001B[0;32m    793\u001B[0m     \u001B[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001B[39;00m\n\u001B[0;32m    794\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_pickle:\n\u001B[1;32m--> 795\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mObject arrays cannot be loaded when \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    796\u001B[0m                          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_pickle=False\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    797\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pickle_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    798\u001B[0m         pickle_kwargs \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[1;31mValueError\u001B[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
